{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"reddit_wsb.csv\")\n",
    "\n",
    "df = df.dropna(subset=['body'])\n",
    "\n",
    "df['document'] = (df['title'].fillna('') + ' ' + df['body']).str.strip()\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "documents = df['document'].tolist()\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def normalize(text):\n",
    "    text = text.lower()                        # lowercase\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)   # remove punctuation/symbols\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()   # collapse spaces\n",
    "    return text\n",
    "\n",
    "documents = [normalize(doc) for doc in documents]\n",
    "\n",
    "# Sample Normalized Document\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import hashlib\n",
    "\n",
    "def shingle (text, k):\n",
    "    \"\"\"Generate k-shingles from the input text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to generate shingles from.\n",
    "        k (int): The length of each shingle.\n",
    "\n",
    "    Returns:\n",
    "        set: A set of k-shingles.\n",
    "    \"\"\"\n",
    "    shingles = set()\n",
    "    text_length = len(text)\n",
    "    for i in range(text_length - k + 1):\n",
    "        shingle = text[i:i + k]\n",
    "        shingles.add(shingle)\n",
    "    return shingles\n",
    "\n",
    "def stable_hash64(s: str) -> int:\n",
    "    \"\"\"Deterministic 64-bit integer hash from a string using SHA-1 (first 8 bytes).\"\"\"\n",
    "    h = hashlib.sha1(s.encode('utf-8')).digest()\n",
    "    return int.from_bytes(h[:8], 'big')\n",
    "\n",
    "def hashShingle (text, k):\n",
    "    \"\"\"Generate hashed k-shingles from the input text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to generate hashed shingles from.\n",
    "        k (int): The length of each shingle.\n",
    "\n",
    "    Returns:\n",
    "        set: A set of hashed k-shingles.\n",
    "    \"\"\"\n",
    "    shingles = shingle(text, k)\n",
    "    hashed_shingles = set()\n",
    "    for sh in shingles:\n",
    "        hashed_shingles.add(stable_hash64(sh))\n",
    "    return sorted(hashed_shingles)\n",
    "\n",
    "shingle_sets = {}\n",
    "for i, doc in enumerate(documents, start=1):\n",
    "    shingles = hashShingle(doc, 5)\n",
    "    if shingles:  # non-empty set\n",
    "        shingle_sets[f\"doc{i}\"] = shingles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccardSimilarity (j1, j2):\n",
    "    \"\"\"Calculate the Jaccard similarity between two sets.\n",
    "\n",
    "    Args:\n",
    "        set1 (set): The first set.\n",
    "        set2 (set): The second set.\n",
    "\n",
    "    Returns:\n",
    "        float: The Jaccard similarity between the two sets.\n",
    "    \"\"\"\n",
    "    set1 = set(j1)\n",
    "    set2 = set(j2)\n",
    "    intersection = set1.intersection(set2)\n",
    "    union = set1.union(set2)\n",
    "    if not union:\n",
    "        return 0.0\n",
    "    return len(intersection) / len(union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "p = 2**61 - 1\n",
    "num_hashes = 100\n",
    "\n",
    "a = np.random.randint(1, p, size=num_hashes, dtype=np.uint64)\n",
    "b = np.random.randint(0, p, size=num_hashes, dtype=np.uint64)\n",
    "\n",
    "def minHash_vectorized(shingle_set):\n",
    "    # Convert shingle_set to a NumPy array for vector math\n",
    "    x = np.array(list(shingle_set), dtype=np.uint64)\n",
    "\n",
    "    # Broadcasted hashing:\n",
    "    # For each hash function (row), apply (a*x + b) % p to all shingles\n",
    "    hashes = (a[:, None] * x[None, :] + b[:, None]) % p\n",
    "\n",
    "    # Take the minimum per row (i.e., per hash function)\n",
    "    signatures = np.min(hashes, axis=1)\n",
    "\n",
    "    return signatures.tolist()\n",
    "\n",
    "# Apply to all documents\n",
    "minhash_signatures = {k: minHash_vectorized(v) for k, v in shingle_sets.items()}\n",
    "\n",
    "print(minhash_signatures[\"doc1\"])  # Example output of minhash signature for document 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareSignatures (m1, m2):\n",
    "    agree = 0\n",
    "    for i in range(len(m1)):\n",
    "        if m1[i] == m2[i]:\n",
    "            agree += 1\n",
    "    return agree / len(m1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "\n",
    "def lsh_candidates(signatures, bands, rows_per_band):\n",
    "    doc_ids = list(signatures.keys())\n",
    "    sig_matrix = np.array(list(signatures.values()), dtype=np.uint64)\n",
    "    assert sig_matrix.shape[1] == bands * rows_per_band\n",
    "\n",
    "    buckets = [defaultdict(list) for _ in range(bands)]\n",
    "\n",
    "    # Precompute slices for all bands\n",
    "    for band in range(bands):\n",
    "        start = band * rows_per_band\n",
    "        end = start + rows_per_band\n",
    "        band_slice = sig_matrix[:, start:end]\n",
    "\n",
    "        # Efficient string join instead of str(tuple(...))\n",
    "        # Much faster, same deterministic content\n",
    "        for i, row in enumerate(band_slice):\n",
    "            s = ','.join(map(str, row))\n",
    "            bucket_hash = stable_hash64(s)\n",
    "            buckets[band][bucket_hash].append(doc_ids[i])\n",
    "\n",
    "    # Generate candidate pairs\n",
    "    candidates = set()\n",
    "    for band_dict in buckets:\n",
    "        for doc_list in band_dict.values():\n",
    "            if len(doc_list) > 1:\n",
    "                doc_list = sorted(set(doc_list))\n",
    "                candidates.update(combinations(doc_list, 2))\n",
    "\n",
    "    return candidates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_candidates(signatures, candidate_pairs, threshold=0.8, verbose=False):\n",
    "    \"\"\"\n",
    "    Compare candidate pairs efficiently and filter by similarity threshold.\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"\\n=== STEP 5: OFFICIAL CANDIDATE COMPARISON ===\")\n",
    "        print(f\"Similarity threshold: {threshold}\\n\")\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # Skip sorting (saves memory)\n",
    "    for i, (d1, d2) in enumerate(candidate_pairs):\n",
    "        sig1, sig2 = signatures[d1], signatures[d2]\n",
    "        matches = sum(s1 == s2 for s1, s2 in zip(sig1, sig2))\n",
    "        similarity = matches / len(sig1)\n",
    "\n",
    "        if similarity >= threshold:\n",
    "            results[(d1, d2)] = similarity\n",
    "\n",
    "        # Lightweight progress print\n",
    "        if verbose and i % 1000 == 0:\n",
    "            print(f\"Processed {i} pairs... current matches: {len(results)}\")\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\nTotal qualifying pairs: {len(results)}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "signatures = minhash_signatures\n",
    "\n",
    "# Calculates candidate pairs based on LSH\n",
    "# Messing around with bands and rows_per_band heavily changes the number of candidates\n",
    "candidates = lsh_candidates(signatures, bands=4, rows_per_band=25)\n",
    "\n",
    "# Compares candidate pairs and accepts only those above the threshold\n",
    "similar_pairs = compare_candidates(signatures, candidates, threshold=0.8, verbose=True)\n",
    "\n",
    "print(len(similar_pairs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) How many docs total and how many unique signatures?\n",
    "num_docs = len(signatures)\n",
    "unique_sigs = len({tuple(sig) for sig in signatures.values()})\n",
    "print(\"docs:\", num_docs, \"unique signatures:\", unique_sigs,\n",
    "      f\"({unique_sigs/num_docs:.2%} unique)\")\n",
    "\n",
    "# 2) Are any signatures literally identical objects? (accidental list * n bug)\n",
    "same_object_count = len([1 for s in signatures.values() if id(s) == id(next(iter(signatures.values())))])\n",
    "print(\"Example identical-object check (should be 1):\", same_object_count)\n",
    "\n",
    "# 3) Check signature length and a quick per-position entropy-ish check\n",
    "sig_len = len(next(iter(signatures.values())))\n",
    "print(\"signature length:\", sig_len)\n",
    "\n",
    "# per-position distinct counts (spot collisions)\n",
    "from collections import Counter\n",
    "pos_counters = [Counter() for _ in range(sig_len)]\n",
    "for sig in signatures.values():\n",
    "    for i, v in enumerate(sig):\n",
    "        pos_counters[i][v] += 1\n",
    "distinct_counts = [len(c) for c in pos_counters]\n",
    "print(\"distinct values per position (first 10):\", distinct_counts[:10])\n",
    "print(\"min/median/max distinct per position:\", min(distinct_counts), sorted(distinct_counts)[len(distinct_counts)//2], max(distinct_counts))\n",
    "\n",
    "# 4) Sample pairwise similarities (random small sample)\n",
    "import random\n",
    "pairs = []\n",
    "docs = list(signatures.keys())\n",
    "for _ in range(1000):\n",
    "    a,b = random.sample(docs, 2)\n",
    "    sigA, sigB = signatures[a], signatures[b]\n",
    "    matches = sum(1 for x,y in zip(sigA,sigB) if x==y)\n",
    "    pairs.append(matches/len(sigA))\n",
    "import statistics\n",
    "print(\"sample similarity: mean\", statistics.mean(pairs), \"median\", statistics.median(pairs),\n",
    "      \"90th pct\", sorted(pairs)[int(.9*len(pairs))])\n",
    "\n",
    "\n",
    "import statistics\n",
    "\n",
    "sims = list(similar_pairs.values())\n",
    "print(\"min\", min(sims), \"median\", statistics.median(sims),\n",
    "      \"max\", max(sims))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-mining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
