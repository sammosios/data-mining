{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"reddit_wsb.csv\")\n",
    "\n",
    "df = df.dropna(subset=['body'])\n",
    "\n",
    "df['document'] = (df['title'].fillna('') + ' ' + df['body']).str.strip()\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "documents = df['document'].tolist()\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def normalize(text):\n",
    "    text = text.lower()                        # lowercase\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)   # remove punctuation/symbols\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()   # collapse spaces\n",
    "    return text\n",
    "\n",
    "documents = [normalize(doc) for doc in documents]\n",
    "\n",
    "# Sample Normalized Document\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import hashlib\n",
    "\n",
    "def shingle (text, k):\n",
    "    \"\"\"Generate k-shingles from the input text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to generate shingles from.\n",
    "        k (int): The length of each shingle.\n",
    "\n",
    "    Returns:\n",
    "        set: A set of k-shingles.\n",
    "    \"\"\"\n",
    "    shingles = set()\n",
    "    text_length = len(text)\n",
    "    for i in range(text_length - k + 1):\n",
    "        shingle = text[i:i + k]\n",
    "        shingles.add(shingle)\n",
    "    return shingles\n",
    "\n",
    "def stable_hash64(s: str) -> int:\n",
    "    \"\"\"Deterministic 64-bit integer hash from a string using SHA-1 (first 8 bytes).\"\"\"\n",
    "    h = hashlib.sha1(s.encode('utf-8')).digest()\n",
    "    return int.from_bytes(h[:8], 'big')\n",
    "\n",
    "def hashShingle (text, k):\n",
    "    \"\"\"Generate hashed k-shingles from the input text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to generate hashed shingles from.\n",
    "        k (int): The length of each shingle.\n",
    "\n",
    "    Returns:\n",
    "        set: A set of hashed k-shingles.\n",
    "    \"\"\"\n",
    "    shingles = shingle(text, k)\n",
    "    hashed_shingles = set()\n",
    "    for sh in shingles:\n",
    "        hashed_shingles.add(stable_hash64(sh))\n",
    "    return sorted(hashed_shingles)\n",
    "\n",
    "\n",
    "shingle_sets = {}\n",
    "for i, doc in enumerate(documents, start=1):\n",
    "    key = f\"doc{i}\"\n",
    "    shingle_sets[key] = hashShingle(doc, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccardSimilarity (j1, j2):\n",
    "    \"\"\"Calculate the Jaccard similarity between two sets.\n",
    "\n",
    "    Args:\n",
    "        set1 (set): The first set.\n",
    "        set2 (set): The second set.\n",
    "\n",
    "    Returns:\n",
    "        float: The Jaccard similarity between the two sets.\n",
    "    \"\"\"\n",
    "    set1 = set(j1)\n",
    "    set2 = set(j2)\n",
    "    intersection = set1.intersection(set2)\n",
    "    union = set1.union(set2)\n",
    "    if not union:\n",
    "        return 0.0\n",
    "    return len(intersection) / len(union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 2**61 - 1\n",
    "a = [random.randint(1, p-1) for _ in range(100)]\n",
    "b = [random.randint(0, p-1) for _ in range(100)]\n",
    "\n",
    "def hash_i(i, x):\n",
    "    return (a[i] * (x) + b[i]) % p\n",
    "\n",
    "def minHash(shingle, numHashes):\n",
    "    signatures = []\n",
    "    for i in range(numHashes):\n",
    "        minHash = float('inf')\n",
    "        for sh in shingle:\n",
    "            hashCode = hash_i(i, sh)\n",
    "            if hashCode < minHash:\n",
    "                minHash = hashCode\n",
    "        signatures.append(minHash)\n",
    "    return signatures\n",
    "\n",
    "minhash_signatures = {k: minHash(v, 100) for k, v in shingle_sets.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareSignatures (m1, m2):\n",
    "    agree = 0\n",
    "    for i in range(len(m1)):\n",
    "        if m1[i] == m2[i]:\n",
    "            agree += 1\n",
    "    return agree / len(m1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "\n",
    "def lsh_candidates_debug(signatures, bands, rows_per_band):\n",
    "    \"\"\"\n",
    "    signatures: dict of {doc_id: [signature_values]}\n",
    "    bands: number of bands (b)\n",
    "    rows_per_band: number of rows per band (r)\n",
    "\n",
    "    Returns:\n",
    "        set of candidate document pairs (tuples)\n",
    "    \"\"\"\n",
    "    assert all(len(sig) == bands * rows_per_band for sig in signatures.values()), \\\n",
    "        \"Signature length must equal bands * rows_per_band\"\n",
    "\n",
    "    buckets = [defaultdict(list) for _ in range(bands)]\n",
    "    candidates = set()\n",
    "\n",
    "    print(\"=== STEP 1: SPLIT SIGNATURES INTO BANDS AND HASH ===\")\n",
    "    for doc_id, sig in signatures.items():\n",
    "        print(f\"\\nDocument: {doc_id}\")\n",
    "        for band in range(bands):\n",
    "            start = band * rows_per_band\n",
    "            end = start + rows_per_band\n",
    "            band_slice = tuple(sig[start:end])\n",
    "            bucket_hash = stable_hash64(str(band_slice))\n",
    "            buckets[band][bucket_hash].append(doc_id)\n",
    "            print(f\"  Band {band}: {band_slice} → hash={bucket_hash % 1000} (added to bucket {band})\")\n",
    "\n",
    "    print(\"\\n=== STEP 2: BUCKET CONTENTS ===\")\n",
    "    for band, band_dict in enumerate(buckets):\n",
    "        print(f\"\\nBand {band}:\")\n",
    "        for h, docs in band_dict.items():\n",
    "            print(f\"  Bucket hash {h % 1000}: {docs}\")\n",
    "\n",
    "    print(\"\\n=== STEP 3: CANDIDATE GENERATION ===\")\n",
    "    for band, band_dict in enumerate(buckets):\n",
    "        for doc_list in band_dict.values():\n",
    "            if len(doc_list) > 1:\n",
    "                for d1, d2 in combinations(sorted(set(doc_list)), 2):\n",
    "                    candidates.add((d1, d2))\n",
    "                    print(f\"  Band {band}: {d1} and {d2} share a bucket → candidate pair\")\n",
    "\n",
    "    print(\"\\n=== STEP 4: FINAL CANDIDATES ===\")\n",
    "    if candidates:\n",
    "        for c in sorted(candidates):\n",
    "            print(f\"  {c}\")\n",
    "    else:\n",
    "        print(\"  No candidate pairs found.\")\n",
    "\n",
    "    return candidates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_candidates(signatures, candidate_pairs, threshold=0.8):\n",
    "    \"\"\"\n",
    "    Compare candidate document pairs and filter those that meet the similarity threshold.\n",
    "    \n",
    "    Args:\n",
    "        signatures (dict): {doc_id: [signature_values]}\n",
    "        candidate_pairs (set): set of (doc1, doc2) tuples from LSH\n",
    "        threshold (float): minimum similarity fraction (0–1)\n",
    "        \n",
    "    Returns:\n",
    "        dict: { (doc1, doc2): similarity_value } for pairs meeting threshold\n",
    "    \"\"\"\n",
    "    print(\"\\n=== STEP 5: OFFICIAL CANDIDATE COMPARISON ===\")\n",
    "    print(f\"Similarity threshold: {threshold}\\n\")\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for d1, d2 in sorted(candidate_pairs):\n",
    "        sig1, sig2 = signatures[d1], signatures[d2]\n",
    "        assert len(sig1) == len(sig2), \"Signatures must be of the same length\"\n",
    "        \n",
    "        # Count matching components\n",
    "        matches = sum(1 for i in range(len(sig1)) if sig1[i] == sig2[i])\n",
    "        similarity = matches / len(sig1)\n",
    "        \n",
    "        status = \"✅ KEPT\" if similarity >= threshold else \"❌ REJECTED\"\n",
    "        print(f\"{d1}-{d2}: {matches}/{len(sig1)} components match → similarity={similarity:.3f} → {status}\")\n",
    "        \n",
    "        if similarity >= threshold:\n",
    "            results[(d1, d2)] = similarity\n",
    "\n",
    "    if not results:\n",
    "        print(\"\\nNo pairs met the similarity threshold.\")\n",
    "    else:\n",
    "        print(\"\\n=== STEP 6: FINAL SIMILAR DOCUMENT PAIRS ===\")\n",
    "        for (d1, d2), sim in results.items():\n",
    "            print(f\"  ({d1}, {d2}) → similarity={sim:.3f}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "signatures = minhash_signatures\n",
    "\n",
    "# Calculates candidate pairs based on LSH\n",
    "candidates = lsh_candidates_debug(signatures, bands=20, rows_per_band=5)\n",
    "\n",
    "# Compares candidate pairs and accepts only those above the threshold\n",
    "similar_pairs = compare_candidates(signatures, candidates, threshold=0.8)\n",
    "\n",
    "print(len(similar_pairs))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-mining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
