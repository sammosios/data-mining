{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 24738 documents\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"reddit_wsb.csv\")\n",
    "\n",
    "df = df.dropna(subset=['body'])\n",
    "\n",
    "df['document'] = (df['title'].fillna('') + ' ' + df['body']).str.strip()\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "documents = df['document'].tolist()\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exit the system the ceo of nasdaq pushed to halt trading to give investors a chance to recalibrate their positions https mobile twitter com mediaite status 1354504710695362563 https mobile twitter com mediaite status 1354504710695362563 now sec is investigating brokers are disallowing buying more calls this is the institutions flat out admitting they will change the rules to bail out the rich but if it happens to us we get a well shucks you should have known investing is risky have you tried cutting out avocados and coffee maybe doing uber on the side we may have collectively driven up enough sentiment in wall street to make other big players go long on gme with us we do not have the money to move the stock as much as it did alone we didn t hurt wall street as a whole just a few funds went down while others went up and profited off the shorts the same as us the media wants to pin the blame on us it should be crystal clear that this is a rigged game by now its time to build new exchanges that can t arbitrarily change the rules on us cr o has some version of these maybe they can be repurposed to be trade stock without government intervention i don t know exactly what it will look like yet but the broad next steps i see are 1 exit the current financial system 2 build a new one\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def normalize(text):\n",
    "    text = text.lower()                        # lowercase\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)   # remove punctuation/symbols\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()   # collapse spaces\n",
    "    return text\n",
    "\n",
    "documents = [normalize(doc) for doc in documents]\n",
    "\n",
    "sample_100 = documents[:100]\n",
    "sample_1000 = documents[:1000]\n",
    "sample_10000 = documents[:10000]\n",
    "\n",
    "# Sample Normalized Document\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import hashlib\n",
    "\n",
    "def shingle (text, k):\n",
    "    \"\"\"Generate k-shingles from the input text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to generate shingles from.\n",
    "        k (int): The length of each shingle.\n",
    "\n",
    "    Returns:\n",
    "        set: A set of k-shingles.\n",
    "    \"\"\"\n",
    "    shingles = set()\n",
    "    text_length = len(text)\n",
    "    for i in range(text_length - k + 1):\n",
    "        shingle = text[i:i + k]\n",
    "        shingles.add(shingle)\n",
    "    return shingles\n",
    "\n",
    "def stable_hash64(s: str) -> int:\n",
    "    \"\"\"Deterministic 64-bit integer hash from a string using SHA-1 (first 8 bytes).\"\"\"\n",
    "    h = hashlib.sha1(s.encode('utf-8')).digest()\n",
    "    return int.from_bytes(h[:8], 'big')\n",
    "\n",
    "def hashShingle (text, k):\n",
    "    \"\"\"Generate hashed k-shingles from the input text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to generate hashed shingles from.\n",
    "        k (int): The length of each shingle.\n",
    "\n",
    "    Returns:\n",
    "        set: A set of hashed k-shingles.\n",
    "    \"\"\"\n",
    "    shingles = shingle(text, k)\n",
    "    hashed_shingles = set()\n",
    "    for sh in shingles:\n",
    "        hashed_shingles.add(stable_hash64(sh))\n",
    "    return sorted(hashed_shingles)\n",
    "\n",
    "shingle_sets = {}\n",
    "for i, doc in enumerate(documents, start=1):\n",
    "    shingles = hashShingle(doc, 5)\n",
    "    if shingles:  # non-empty set\n",
    "        shingle_sets[f\"doc{i}\"] = shingles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "11d5fbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample100_shingle_sets = {}\n",
    "for i, doc in enumerate(sample_100, start=1):\n",
    "    shingles = hashShingle(doc, 5)\n",
    "    if shingles:  # non-empty set\n",
    "        sample100_shingle_sets[f\"doc{i}\"] = shingles\n",
    "\n",
    "sample1000_shingle_sets = {}\n",
    "for i, doc in enumerate(sample_1000, start=1):\n",
    "    shingles = hashShingle(doc, 5)\n",
    "    if shingles:  # non-empty set\n",
    "        sample1000_shingle_sets[f\"doc{i}\"] = shingles\n",
    "\n",
    "sample10000_shingle_sets = {}\n",
    "for i, doc in enumerate(sample_10000, start=1):     \n",
    "    shingles = hashShingle(doc, 5)\n",
    "    if shingles:  # non-empty set\n",
    "        sample10000_shingle_sets[f\"doc{i}\"] = shingles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccardSimilarity (j1, j2):\n",
    "    \"\"\"Calculate the Jaccard similarity between two sets.\n",
    "\n",
    "    Args:\n",
    "        set1 (set): The first set.\n",
    "        set2 (set): The second set.\n",
    "\n",
    "    Returns:\n",
    "        float: The Jaccard similarity between the two sets.\n",
    "    \"\"\"\n",
    "    set1 = set(j1)\n",
    "    set2 = set(j2)\n",
    "    intersection = set1.intersection(set2)\n",
    "    union = set1.union(set2)\n",
    "    if not union:\n",
    "        return 0.0\n",
    "    return len(intersection) / len(union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[345089397602898, 2567974316486846, 3080072189639001, 3621578707448673, 620606878809477, 4322113570144494, 2372926116229249, 290955566547970, 1709157416065581, 2223315077028049, 248004311737834, 1632258557387183, 335126648437888, 7086949424797277, 194653819878190, 6048330392162801, 2513145045145986, 564396962657371, 183101369960174, 1906890662224196, 4217464263895757, 4667167192987965, 569015621626730, 358849543945644, 442077367808197, 1296410657099288, 416772951070937, 541889611012223, 1886254666446940, 408566471869421, 108848889045944, 7113398561100318, 234624603408151, 2483608996832255, 1300276538962330, 2269405630379042, 3614216203741224, 5476562412989528, 1106574343516144, 6085346997284083, 2495858697633621, 3461560054030472, 5931995726880241, 5535099076886260, 1823332974900962, 23075505049288, 2036555750494537, 248740859180053, 585931816025459, 3425528873223348, 600033553340304, 814567622128715, 164706312991875, 590610406884094, 1655171496247450, 2551147844553372, 1032648933375020, 507370966169771, 3303662697250467, 859302697932282, 5525461457177734, 971941996473613, 3556770791308415, 2248635872968892, 827341439695865, 5302101037792096, 1176775733842034, 441548783064828, 1767819797138187, 45799926920097, 130927232456820, 76324741965151, 11806576599175585, 241369813666644, 1324632279727179, 1840110998200099, 3294489000816435, 971156235789071, 2635657331469660, 722583583193832, 3309521825985368, 730857238338725, 1455541847406751, 2824544955159737, 864834030656373, 3596436882400083, 763690956042216, 659497496218773, 3834624789686882, 510917156309457, 795124238886557, 2627356449489766, 2284391571701515, 192312837967588, 2432545505854149, 3064747336344162, 2518321778428923, 449454876019807, 937643457996565, 1276258695591142]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "p = 2**61 - 1\n",
    "num_hashes = 100\n",
    "\n",
    "a = np.random.randint(1, p, size=num_hashes, dtype=np.uint64)\n",
    "b = np.random.randint(0, p, size=num_hashes, dtype=np.uint64)\n",
    "\n",
    "def minHash_vectorized(shingle_set):\n",
    "    # Convert shingle_set to a NumPy array for vector math\n",
    "    x = np.array(list(shingle_set), dtype=np.uint64)\n",
    "\n",
    "    # Broadcasted hashing:\n",
    "    # For each hash function (row), apply (a*x + b) % p to all shingles\n",
    "    hashes = (a[:, None] * x[None, :] + b[:, None]) % p\n",
    "\n",
    "    # Take the minimum per row (i.e., per hash function)\n",
    "    signatures = np.min(hashes, axis=1)\n",
    "\n",
    "    return signatures.tolist()\n",
    "\n",
    "# Apply to all documents\n",
    "minhash_signatures = {k: minHash_vectorized(v) for k, v in shingle_sets.items()}\n",
    "\n",
    "print(minhash_signatures[\"doc1\"])  # Example output of minhash signature for document 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareSignatures (m1, m2):\n",
    "    agree = 0\n",
    "    for i in range(len(m1)):\n",
    "        if m1[i] == m2[i]:\n",
    "            agree += 1\n",
    "    return agree / len(m1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "\n",
    "def lsh_candidates(signatures, bands, rows_per_band):\n",
    "    doc_ids = list(signatures.keys())\n",
    "    sig_matrix = np.array(list(signatures.values()), dtype=np.uint64)\n",
    "    assert sig_matrix.shape[1] == bands * rows_per_band\n",
    "\n",
    "    buckets = [defaultdict(list) for _ in range(bands)]\n",
    "\n",
    "    # Precompute slices for all bands\n",
    "    for band in range(bands):\n",
    "        start = band * rows_per_band\n",
    "        end = start + rows_per_band\n",
    "        band_slice = sig_matrix[:, start:end]\n",
    "\n",
    "        # Efficient string join instead of str(tuple(...))\n",
    "        # Much faster, same deterministic content\n",
    "        for i, row in enumerate(band_slice):\n",
    "            s = ','.join(map(str, row))\n",
    "            bucket_hash = stable_hash64(s)\n",
    "            buckets[band][bucket_hash].append(doc_ids[i])\n",
    "\n",
    "    # Generate candidate pairs\n",
    "    candidates = set()\n",
    "    for band_dict in buckets:\n",
    "        for doc_list in band_dict.values():\n",
    "            if len(doc_list) > 1:\n",
    "                doc_list = sorted(set(doc_list))\n",
    "                candidates.update(combinations(doc_list, 2))\n",
    "\n",
    "    return candidates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STEP 5: OFFICIAL CANDIDATE COMPARISON ===\n",
      "Similarity threshold: 0.8\n",
      "\n",
      "Processed 0 pairs... current matches: 1\n",
      "Processed 1000 pairs... current matches: 832\n",
      "Processed 2000 pairs... current matches: 1697\n",
      "Processed 3000 pairs... current matches: 2556\n",
      "Processed 4000 pairs... current matches: 3420\n",
      "Processed 5000 pairs... current matches: 4290\n",
      "Processed 6000 pairs... current matches: 5141\n",
      "Processed 7000 pairs... current matches: 5980\n",
      "Processed 8000 pairs... current matches: 6817\n",
      "Processed 9000 pairs... current matches: 7670\n",
      "Processed 10000 pairs... current matches: 8530\n",
      "Processed 11000 pairs... current matches: 9384\n",
      "Processed 12000 pairs... current matches: 10220\n",
      "Processed 13000 pairs... current matches: 11034\n",
      "Processed 14000 pairs... current matches: 11884\n",
      "Processed 15000 pairs... current matches: 12714\n",
      "Processed 16000 pairs... current matches: 13528\n",
      "Processed 17000 pairs... current matches: 14362\n",
      "Processed 18000 pairs... current matches: 15224\n",
      "Processed 19000 pairs... current matches: 16065\n",
      "Processed 20000 pairs... current matches: 16913\n",
      "Processed 21000 pairs... current matches: 17756\n",
      "Processed 22000 pairs... current matches: 18580\n",
      "Processed 23000 pairs... current matches: 19414\n",
      "Processed 24000 pairs... current matches: 20246\n",
      "Processed 25000 pairs... current matches: 21078\n",
      "Processed 26000 pairs... current matches: 21937\n",
      "Processed 27000 pairs... current matches: 22781\n",
      "Processed 28000 pairs... current matches: 23634\n",
      "\n",
      "Total qualifying pairs: 24488\n",
      "28982\n",
      "24488\n"
     ]
    }
   ],
   "source": [
    "def compare_candidates(signatures, candidate_pairs, threshold=0.8, verbose=False):\n",
    "    \"\"\"\n",
    "    Compare candidate pairs efficiently and filter by similarity threshold.\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"\\n=== STEP 5: OFFICIAL CANDIDATE COMPARISON ===\")\n",
    "        print(f\"Similarity threshold: {threshold}\\n\")\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # Skip sorting (saves memory)\n",
    "    for i, (d1, d2) in enumerate(candidate_pairs):\n",
    "        sig1, sig2 = signatures[d1], signatures[d2]\n",
    "        similarity = compareSignatures(sig1, sig2)\n",
    "\n",
    "        if similarity >= threshold:\n",
    "            results[(d1, d2)] = similarity\n",
    "\n",
    "        # Lightweight progress print\n",
    "        if verbose and i % 1000 == 0:\n",
    "            print(f\"Processed {i} pairs... current matches: {len(results)}\")\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\nTotal qualifying pairs: {len(results)}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "signatures = minhash_signatures\n",
    "\n",
    "# Calculates candidate pairs based on LSH\n",
    "# Messing around with bands and rows_per_band heavily changes the number of candidates\n",
    "candidates = lsh_candidates(signatures, bands=10, rows_per_band=10)\n",
    "\n",
    "# Compares candidate pairs and accepts only those above the threshold\n",
    "similar_pairs = compare_candidates(signatures, candidates, threshold=0.8, verbose=True)\n",
    "\n",
    "print(len(candidates))\n",
    "print(len(similar_pairs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "94eedaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample100_minhash_signatures = {k: minHash_vectorized(v) for k, v in sample100_shingle_sets.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ac1689c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STEP 5: OFFICIAL CANDIDATE COMPARISON ===\n",
      "Similarity threshold: 0.8\n",
      "\n",
      "Processed 0 pairs... current matches: 0\n",
      "\n",
      "Total qualifying pairs: 1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "sample100_signature = sample100_minhash_signatures\n",
    "\n",
    "sample100_candidates = lsh_candidates(sample100_signature, bands=25, rows_per_band=4)\n",
    "\n",
    "similar_pairs_100 = compare_candidates(sample100_signature, sample100_candidates, threshold=0.8, verbose=True)\n",
    "\n",
    "print(len(similar_pairs_100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ef1f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "2745\n"
     ]
    }
   ],
   "source": [
    "#Brute force Jaccard similarity \n",
    "\n",
    "results = {}\n",
    "for i in range(len(sample_100)):\n",
    "    print(i)\n",
    "    doc1 = sample_100[i]\n",
    "    for j in range(i + 1, len(sample_100)):\n",
    "        doc2 = sample_100[j]\n",
    "        similarity = jaccardSimilarity(doc1, doc2)\n",
    "        if (similarity >= 0.8):\n",
    "            results[(i, j)] = similarity\n",
    "print(len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs: 24730 unique signatures: 24445 (98.85% unique)\n",
      "Example identical-object check (should be 1): 1\n",
      "signature length: 100\n",
      "distinct values per position (first 10): [2580, 2601, 2965, 2721, 2531, 2830, 2316, 2365, 2564, 2704]\n",
      "min/median/max distinct per position: 1796 2584 3066\n",
      "sample similarity: mean 0.029580000000000002 median 0.02 90th pct 0.07\n",
      "min 0.81 median 0.97 max 1.0\n"
     ]
    }
   ],
   "source": [
    "# 1) How many docs total and how many unique signatures?\n",
    "num_docs = len(signatures)\n",
    "unique_sigs = len({tuple(sig) for sig in signatures.values()})\n",
    "print(\"docs:\", num_docs, \"unique signatures:\", unique_sigs,\n",
    "      f\"({unique_sigs/num_docs:.2%} unique)\")\n",
    "\n",
    "# 2) Are any signatures literally identical objects? (accidental list * n bug)\n",
    "same_object_count = len([1 for s in signatures.values() if id(s) == id(next(iter(signatures.values())))])\n",
    "print(\"Example identical-object check (should be 1):\", same_object_count)\n",
    "\n",
    "# 3) Check signature length and a quick per-position entropy-ish check\n",
    "sig_len = len(next(iter(signatures.values())))\n",
    "print(\"signature length:\", sig_len)\n",
    "\n",
    "# per-position distinct counts (spot collisions)\n",
    "from collections import Counter\n",
    "pos_counters = [Counter() for _ in range(sig_len)]\n",
    "for sig in signatures.values():\n",
    "    for i, v in enumerate(sig):\n",
    "        pos_counters[i][v] += 1\n",
    "distinct_counts = [len(c) for c in pos_counters]\n",
    "print(\"distinct values per position (first 10):\", distinct_counts[:10])\n",
    "print(\"min/median/max distinct per position:\", min(distinct_counts), sorted(distinct_counts)[len(distinct_counts)//2], max(distinct_counts))\n",
    "\n",
    "# 4) Sample pairwise similarities (random small sample)\n",
    "import random\n",
    "pairs = []\n",
    "docs = list(signatures.keys())\n",
    "for _ in range(1000):\n",
    "    a,b = random.sample(docs, 2)\n",
    "    sigA, sigB = signatures[a], signatures[b]\n",
    "    matches = sum(1 for x,y in zip(sigA,sigB) if x==y)\n",
    "    pairs.append(matches/len(sigA))\n",
    "import statistics\n",
    "print(\"sample similarity: mean\", statistics.mean(pairs), \"median\", statistics.median(pairs),\n",
    "      \"90th pct\", sorted(pairs)[int(.9*len(pairs))])\n",
    "\n",
    "\n",
    "import statistics\n",
    "\n",
    "sims = list(similar_pairs.values())\n",
    "print(\"min\", min(sims), \"median\", statistics.median(sims),\n",
    "      \"max\", max(sims))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e292c7df",
   "metadata": {},
   "source": [
    "**Introduction**\n",
    "\n",
    "In this project, we implemented the neccessary functionality for efficiently combing through a dataset of text documents, and being able to identify and record pairs of documents that are similar to each other. Brute force checking all possible pairs in a dataset to try and identify similar pairs is not a viable strategy, as the n(n-1)/2 comparisons needed is O(n^2) and renders this method obsolete on measurably large datasets. The process that we implemented was to first shingle our data, before then creating a min hash signature matrix for our dataset, and then running locality sensitive hashing (LSH) on our signature matrix to be able to identify candidate pairs that are highly likely to be similar above a certain threshold. This process allowed for a much more time efficient method of identifying similar pairs throughout a dataset of documents over standard naive pair checking.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "++ How to run our code and how to be able to pull in our dataset\n",
    "\n",
    "**Shingling**\n",
    "\n",
    "The first step that needed to be done was to shingle our data to be able to identify the parts that make up each of our documents, in order to use those parts as the basis of our document comparison. Shingling allows us to identify all letter sequences of a certain amount that appear in our document, and to use that as the contents of the document to compare to others. Once the shingles of a document are created, each shingle in that document's shingle set is then hashed to allow for improved computational cost, as they can now be represented and compared as numbers rather than as more memory intensive strings, and allows us to preform min-hashing afterwards. Given that our documents were generally around a medium length (reddit posts), we chose the shingle size for our dataset to be 5. When shingling our full dataset (24738 documents), it takes around 20-22 seconds to hash shingle all the documents.\n",
    "\n",
    "**Min-Hashing**\n",
    "\n",
    "Once we have the hashed shingles of each document, we then go about min-hashing each shingle set to obtain the signautre vector for each document, which when put together forms the signature matrix of our dataset. Min-hashing allows us to represent our longer hashed shingle sets, into much shorter representative \"signatures\" of the shingle set, and this allows for much less expensive comparisons between two documents, as we do not need to compute the intersection and union of large sets, and can compare the much shorter signatures instead. Our min-hash works by creating 100 independent hash functions, and having each hash function be applied to each value in the shingle set. \n",
    "\n",
    "For each hash function, the lowest value that was computed across all the hashes is recorded, and the min hash vector for a document is comprised of the 100 \"lowest values\" that each hash function computed. With this, the larger shingle set for each document can be represented by its much smaller 100 value signature vector. The signature vectors for each document are then combined to create the signature matrix of our dataset, which can be used for Locality Snesitive Hashing to be able to identify candidate pairs that might be similar. When min-hashing all of our documents, it takes around 15 seconds to be able to create the signature matrix.\n",
    "\n",
    "**Locality Sensitive Hashing**\n",
    "\n",
    "The final step in our implemented process is to do locality sensitive hashing onto our signature matrix. Locality sensitive hashing splits up the matrix into \"bands\" of rows, where in each band, the columns of our matrix within that band get hashed into buckets. Once this hashing is done across all bands, we can then identify \"candidate pairs\" as any pair of documents where in at least one band, they were hashed to the same bucket (in other words, any two documents where the signature vectors are identical in at least one section of rows). These candidate pairs are then the pairs that actually get compared to then confirm whether or not they are actually similar. LSH significantly reduces how many similarity comparisons we need to do by filtering out the pairs that do not have an exact signature match in any band, and allows us to be able to sift through similar documents on much larger datasets as a result. \n",
    "\n",
    "The band size plays a large factor in LSH, as a larger number of rows per bands (i.e. less bands) causes for less false positives but more false negatives (as now for a pair to be a candidate pair, they must agree exactly over a larger sequence of rows). As a result of this, we tested a few different configurations of band sizes to see which were better suited to find a good balance between low false-positives, and low false-negatives. The different band sizes also caused some different times for how long the LSH took, as with less bands, because the critertia for being a candidate pair is more strict, there are less candidate pairs identified and thus less comparisons needed. For our full dataset, the time it took for LSH to identify similar pairs was from around 1-3.5 seconds depending on our band size.\n",
    "\n",
    "**Testing**\n",
    "\n",
    "The overwehlming benefit of min-hashing/LSH is that it allows for much faster similarity comparisons, and narrows the field of pairs to search through significantly, which allows for much faster run times over a brute-force approach of checking the Jaccard Similarity across all pairs of documents. We set up samples of 100, 1000, 10000, and all of our documents (24738), and identified similar pairs using the brute force method and with the min-hashing + LSH process to compare the time it took to complete processing. We also tested on an LSH with 4 bands, 10 bands, and 20 bands (25, 10, and 5 rows per band respectively) to obersve how much decreasing the band size would increase the processing time:\n",
    "\n",
    "|             |  100     |  1000  |  10000  |  All    |\n",
    "|:-----------:|:--------:|:------:|:-------:|:-------:|\n",
    "| **Na√Øve**   | .2 sec   | 5.4 sec| 779 sec | N/A     |\n",
    "| **4 bands** | < .1 sec | .1 sec | 1.6 sec | 16.7 sec|\n",
    "| **10 bands**| < .1 sec | .6 sec | 4.9 sec | 17.5 sec|\n",
    "| **20 bands**| .1 sec   | 1.0 sec| 7.6 sec | 19.2 sec|\n",
    "\n",
    "The Naive Jaccard method works fine on very small datasets, but given the O(n^2) nature of this approach, the computing time becomes very large as the dataset size increases, going from one fifth of a second to search through 100 documents, to almost 13 minutes for a dataset of 10000. With this method, searching through our whole dataset of nearly 25000 documents to find similar pairs is simply not feasible, and so we can see plainly the benefit of Min-hashing + LSH, as we are able to search for similar pairs through our whole dataset in a reasonable timeframe (16-20 sec). We can also see that slight difference mentioned before in the time the Min-Hashing + LSH approach takes depending on the band size, as for smaller bands sizes (more bands) it takes more time during the LSH portion as more pairs get searched through due to the less strict requirements for being identified as a candidate pair. \n",
    "\n",
    "Given that the band size has an impact on how many pairs are identified as being candidates for similarity, we also tested how many candidates pairs were identified with each of these band sizes, and how many false positives each band size ended up generating once the candidate pairs were checked to see whether they were actually similar:\n",
    "\n",
    "|              | Candidate Pairs | Pairs Found  | False Positive % |\n",
    "|:------------:|:---------------:|:------------:|:----------------:|\n",
    "| **4 Bands**  |      12274      |     12274    |        0         |\n",
    "| **10 Bands** |      28982      |     24448    |      15.64       |\n",
    "| **20 Bands** |      56200      |     25443    |      54.80       |\n",
    "\n",
    "With 4 bands, and a very tight restriction on candidate pairs, we noticed that we did not receive any false positives, and that all candidate pairs turned out to meet our similarity threshold of 80 percent. As the number of bands increased and the band size decreased, we noticed a very significant jump in candidate pairs. Having 10 bands of 10 rows gave us well over double the identified candidate pairs, and almost double the amount of actually similar pairs. This did also come up with the apperance of a sizeable amount of false positives, with 15.64 percent of the candidate pairs not actually being similar. Increasing the band number to 20 (5 rows per band) once again increased the number of candidate pairs by around double the amount, however this time the actual similar pairs did not increase nearly as significantly, gaining slightly less than 1000 actual similar pairs. This resulted in a skyrocketed amount of false positives in this band cnfiguration, as over half of the identified candidate pairs did not end up meeting our similarity threshold, and so the small improvement in identified pairs relative to the spike in false positives renders this configuration not optimal for our dataset. \n",
    "\n",
    "**Conclusions**\n",
    "\n",
    "While brute force Jaccard similarity does identify all pairs that meet a similarity threshold, and can work fine with very small datasets, it proves infeasible quickly. Through our min-hashing + LSH implementation, we can see in our test dataset the impact it has, allowing us to sift through our whole dataset to look for similar pairs of documents, something that was not reasonable with a brute forch approach. We also saw the importance of configuring the band amount in LSH to tailor to what is important when identifying candidate pairs. With our dataset, should ensuring that no identified pair turns out to not be similar is neccessary, than a configuration with 4 bands might work. If making sure that a good amount of the actual similar pairs get identified, with some room for false positives being acceptable, a different configuration of 10 bands might be better suited, and this tuning of the LSH setup also applies outside of our dataset. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-mining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
