{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "path = kagglehub.dataset_download(\"gpreda/reddit-wallstreetsbets-posts\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(f\"{path}/reddit_wsb.csv\")\n",
    "\n",
    "df = df.dropna(subset=['body'])\n",
    "\n",
    "df['document'] = (df['title'].fillna('') + ' ' + df['body']).str.strip()\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "documents = df['document'].tolist()\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def normalize(text):\n",
    "    text = text.lower()                        # lowercase\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)   # remove punctuation/symbols\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()   # collapse spaces\n",
    "    return text\n",
    "\n",
    "documents = [normalize(doc) for doc in documents]\n",
    "\n",
    "sample_100 = documents[:100]\n",
    "sample_1000 = documents[:1000]\n",
    "sample_10000 = documents[:10000]\n",
    "\n",
    "# Sample Normalized Document\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import hashlib\n",
    "\n",
    "def shingle (text, k):\n",
    "    \"\"\"Generate k-shingles from the input text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to generate shingles from.\n",
    "        k (int): The length of each shingle.\n",
    "\n",
    "    Returns:\n",
    "        set: A set of k-shingles.\n",
    "    \"\"\"\n",
    "    shingles = set()\n",
    "    text_length = len(text)\n",
    "    for i in range(text_length - k + 1):\n",
    "        shingle = text[i:i + k]\n",
    "        shingles.add(shingle)\n",
    "    return shingles\n",
    "\n",
    "def stable_hash64(s: str) -> int:\n",
    "    \"\"\"Deterministic 64-bit integer hash from a string using SHA-1 (first 8 bytes).\"\"\"\n",
    "    h = hashlib.sha1(s.encode('utf-8')).digest()\n",
    "    return int.from_bytes(h[:8], 'big')\n",
    "\n",
    "def hashShingle (text, k):\n",
    "    \"\"\"Generate hashed k-shingles from the input text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to generate hashed shingles from.\n",
    "        k (int): The length of each shingle.\n",
    "\n",
    "    Returns:\n",
    "        set: A set of hashed k-shingles.\n",
    "    \"\"\"\n",
    "    shingles = shingle(text, k)\n",
    "    hashed_shingles = set()\n",
    "    for sh in shingles:\n",
    "        hashed_shingles.add(stable_hash64(sh))\n",
    "    return sorted(hashed_shingles)\n",
    "\n",
    "shingle_sets = {}\n",
    "for i, doc in enumerate(documents, start=1):\n",
    "    shingles = hashShingle(doc, 5)\n",
    "    if shingles:  # non-empty set\n",
    "        shingle_sets[f\"doc{i}\"] = shingles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample100_shingle_sets = {}\n",
    "for i, doc in enumerate(sample_100, start=1):\n",
    "    shingles = hashShingle(doc, 5)\n",
    "    if shingles:  # non-empty set\n",
    "        sample100_shingle_sets[f\"doc{i}\"] = shingles\n",
    "\n",
    "sample1000_shingle_sets = {}\n",
    "for i, doc in enumerate(sample_1000, start=1):\n",
    "    shingles = hashShingle(doc, 5)\n",
    "    if shingles:  # non-empty set\n",
    "        sample1000_shingle_sets[f\"doc{i}\"] = shingles\n",
    "\n",
    "sample10000_shingle_sets = {}\n",
    "for i, doc in enumerate(sample_10000, start=1):     \n",
    "    shingles = hashShingle(doc, 5)\n",
    "    if shingles:  # non-empty set\n",
    "        sample10000_shingle_sets[f\"doc{i}\"] = shingles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccardSimilarity (j1, j2):\n",
    "    \"\"\"Calculate the Jaccard similarity between two sets.\n",
    "\n",
    "    Args:\n",
    "        set1 (set): The first set.\n",
    "        set2 (set): The second set.\n",
    "\n",
    "    Returns:\n",
    "        float: The Jaccard similarity between the two sets.\n",
    "    \"\"\"\n",
    "    set1 = set(j1)\n",
    "    set2 = set(j2)\n",
    "    intersection = set1.intersection(set2)\n",
    "    union = set1.union(set2)\n",
    "    if not union:\n",
    "        return 0.0\n",
    "    return len(intersection) / len(union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "p = 2**61 - 1\n",
    "num_hashes = 100\n",
    "\n",
    "a = np.random.randint(1, p, size=num_hashes, dtype=np.uint64)\n",
    "b = np.random.randint(0, p, size=num_hashes, dtype=np.uint64)\n",
    "\n",
    "def minHash_vectorized(shingle_set):\n",
    "    # Convert shingle_set to a NumPy array for vector math\n",
    "    x = np.array(list(shingle_set), dtype=np.uint64)\n",
    "\n",
    "    # Broadcasted hashing:\n",
    "    # For each hash function (row), apply (a*x + b) % p to all shingles\n",
    "    hashes = (a[:, None] * x[None, :] + b[:, None]) % p\n",
    "\n",
    "    # Take the minimum per row (i.e., per hash function)\n",
    "    signatures = np.min(hashes, axis=1)\n",
    "\n",
    "    return signatures.tolist()\n",
    "\n",
    "# Apply to all documents\n",
    "minhash_signatures = {k: minHash_vectorized(v) for k, v in shingle_sets.items()}\n",
    "\n",
    "print(minhash_signatures[\"doc1\"])  # Example output of minhash signature for document 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareSignatures (m1, m2):\n",
    "    agree = 0\n",
    "    for i in range(len(m1)):\n",
    "        if m1[i] == m2[i]:\n",
    "            agree += 1\n",
    "    return agree / len(m1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "\n",
    "def lsh_candidates(signatures, bands, rows_per_band):\n",
    "    doc_ids = list(signatures.keys())\n",
    "    sig_matrix = np.array(list(signatures.values()), dtype=np.uint64)\n",
    "    assert sig_matrix.shape[1] == bands * rows_per_band\n",
    "\n",
    "    buckets = [defaultdict(list) for _ in range(bands)]\n",
    "\n",
    "    # Precompute slices for all bands\n",
    "    for band in range(bands):\n",
    "        start = band * rows_per_band\n",
    "        end = start + rows_per_band\n",
    "        band_slice = sig_matrix[:, start:end]\n",
    "\n",
    "        # Efficient string join instead of str(tuple(...))\n",
    "        # Much faster, same deterministic content\n",
    "        for i, row in enumerate(band_slice):\n",
    "            s = ','.join(map(str, row))\n",
    "            bucket_hash = stable_hash64(s)\n",
    "            buckets[band][bucket_hash].append(doc_ids[i])\n",
    "\n",
    "    # Generate candidate pairs\n",
    "    candidates = set()\n",
    "    for band_dict in buckets:\n",
    "        for doc_list in band_dict.values():\n",
    "            if len(doc_list) > 1:\n",
    "                doc_list = sorted(set(doc_list))\n",
    "                candidates.update(combinations(doc_list, 2))\n",
    "\n",
    "    return candidates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_candidates(signatures, candidate_pairs, threshold=0.8, verbose=False):\n",
    "    \"\"\"\n",
    "    Compare candidate pairs efficiently and filter by similarity threshold.\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"\\n=== STEP 5: OFFICIAL CANDIDATE COMPARISON ===\")\n",
    "        print(f\"Similarity threshold: {threshold}\\n\")\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # Skip sorting (saves memory)\n",
    "    for i, (d1, d2) in enumerate(candidate_pairs):\n",
    "        sig1, sig2 = signatures[d1], signatures[d2]\n",
    "        similarity = compareSignatures(sig1, sig2)\n",
    "\n",
    "        if similarity >= threshold:\n",
    "            results[(d1, d2)] = similarity\n",
    "\n",
    "        # Lightweight progress print\n",
    "        if verbose and i % 1000 == 0:\n",
    "            print(f\"Processed {i} pairs... current matches: {len(results)}\")\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\nTotal qualifying pairs: {len(results)}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "signatures = minhash_signatures\n",
    "\n",
    "# Calculates candidate pairs based on LSH\n",
    "# Messing around with bands and rows_per_band heavily changes the number of candidates\n",
    "candidates = lsh_candidates(signatures, bands=10, rows_per_band=10)\n",
    "\n",
    "# Compares candidate pairs and accepts only those above the threshold\n",
    "similar_pairs = compare_candidates(signatures, candidates, threshold=0.8, verbose=True)\n",
    "\n",
    "print(len(candidates))\n",
    "print(len(similar_pairs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample100_minhash_signatures = {k: minHash_vectorized(v) for k, v in sample100_shingle_sets.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample100_signature = sample100_minhash_signatures\n",
    "\n",
    "sample100_candidates = lsh_candidates(sample100_signature, bands=25, rows_per_band=4)\n",
    "\n",
    "similar_pairs_100 = compare_candidates(sample100_signature, sample100_candidates, threshold=0.8, verbose=True)\n",
    "\n",
    "print(len(similar_pairs_100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Brute force Jaccard similarity \n",
    "\n",
    "results = {}\n",
    "for i in range(len(sample_100)):\n",
    "    print(i)\n",
    "    doc1 = sample_100[i]\n",
    "    for j in range(i + 1, len(sample_100)):\n",
    "        doc2 = sample_100[j]\n",
    "        similarity = jaccardSimilarity(doc1, doc2)\n",
    "        if (similarity >= 0.8):\n",
    "            results[(i, j)] = similarity\n",
    "print(len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) How many docs total and how many unique signatures?\n",
    "num_docs = len(signatures)\n",
    "unique_sigs = len({tuple(sig) for sig in signatures.values()})\n",
    "print(\"docs:\", num_docs, \"unique signatures:\", unique_sigs,\n",
    "      f\"({unique_sigs/num_docs:.2%} unique)\")\n",
    "\n",
    "# 2) Are any signatures literally identical objects? (accidental list * n bug)\n",
    "same_object_count = len([1 for s in signatures.values() if id(s) == id(next(iter(signatures.values())))])\n",
    "print(\"Example identical-object check (should be 1):\", same_object_count)\n",
    "\n",
    "# 3) Check signature length and a quick per-position entropy-ish check\n",
    "sig_len = len(next(iter(signatures.values())))\n",
    "print(\"signature length:\", sig_len)\n",
    "\n",
    "# per-position distinct counts (spot collisions)\n",
    "from collections import Counter\n",
    "pos_counters = [Counter() for _ in range(sig_len)]\n",
    "for sig in signatures.values():\n",
    "    for i, v in enumerate(sig):\n",
    "        pos_counters[i][v] += 1\n",
    "distinct_counts = [len(c) for c in pos_counters]\n",
    "print(\"distinct values per position (first 10):\", distinct_counts[:10])\n",
    "print(\"min/median/max distinct per position:\", min(distinct_counts), sorted(distinct_counts)[len(distinct_counts)//2], max(distinct_counts))\n",
    "\n",
    "# 4) Sample pairwise similarities (random small sample)\n",
    "import random\n",
    "pairs = []\n",
    "docs = list(signatures.keys())\n",
    "for _ in range(1000):\n",
    "    a,b = random.sample(docs, 2)\n",
    "    sigA, sigB = signatures[a], signatures[b]\n",
    "    matches = sum(1 for x,y in zip(sigA,sigB) if x==y)\n",
    "    pairs.append(matches/len(sigA))\n",
    "import statistics\n",
    "print(\"sample similarity: mean\", statistics.mean(pairs), \"median\", statistics.median(pairs),\n",
    "      \"90th pct\", sorted(pairs)[int(.9*len(pairs))])\n",
    "\n",
    "\n",
    "import statistics\n",
    "\n",
    "sims = list(similar_pairs.values())\n",
    "print(\"min\", min(sims), \"median\", statistics.median(sims),\n",
    "      \"max\", max(sims))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## **Instructions**\n",
    "\n",
    "### **Downloading the dataset**\n",
    "\n",
    "\n",
    "The dataset is publicly available on Kaggle at:\n",
    "https://www.kaggle.com/datasets/gpreda/reddit-wallstreetsbets-posts.\n",
    "Executing the first cell will download the dataset into your ```$HOME/.cache``` directory.\n",
    "\n",
    "### **Running the Code**\n",
    "\n",
    "This project uses the ```uv``` package manager. \n",
    "\n",
    "For information on how to install it, visit: https://docs.astral.sh/uv/getting-started/installation/\n",
    "\n",
    "Once ```uv``` is installed, download the project requirements by executing ```uv sync```.\n",
    "Afterwards, you should be ready to run the notebook.\n",
    "The cells should be executed in order, since they usually depend on variables and outputs from previous ones.\n",
    "\n",
    "---\n",
    "\n",
    "## **Report**\n",
    "\n",
    "### **Introduction**\n",
    "\n",
    "In this project, we implemented the neccessary functionality for efficiently combing through a dataset of text documents, and being able to identify and record pairs of documents that are similar to each other. Brute force checking all possible pairs in a dataset to try and identify similar pairs is not a viable strategy, as the n(n-1)/2 comparisons needed is O(n^2) and renders this method obsolete on measurably large datasets. The process that we implemented was to first shingle our data, before then creating a min hash signature matrix for our dataset, and then running locality sensitive hashing (LSH) on our signature matrix to be able to identify candidate pairs that are highly likely to be similar above a certain threshold. This process allowed for a much more time efficient method of identifying similar pairs throughout a dataset of documents over standard naive pair checking.\n",
    "\n",
    "### **Shingling**\n",
    "\n",
    "The first step that needed to be done was to shingle our data to be able to identify the parts that make up each of our documents, in order to use those parts as the basis of our document comparison. Shingling allows us to identify all letter sequences of a certain amount that appear in our document, and to use that as the contents of the document to compare to others. Once the shingles of a document are created, each shingle in that document's shingle set is then hashed to allow for improved computational cost, as they can now be represented and compared as numbers rather than as more memory intensive strings, and allows us to preform min-hashing afterwards. Given that our documents were generally around a medium length (reddit posts), we chose the shingle size for our dataset to be 5. When shingling our full dataset (24738 documents), it takes around 20-22 seconds to hash shingle all the documents.\n",
    "\n",
    "### **Min-Hashing**\n",
    "\n",
    "Once we have the hashed shingles of each document, we then go about min-hashing each shingle set to obtain the signautre vector for each document, which when put together forms the signature matrix of our dataset. Min-hashing allows us to represent our longer hashed shingle sets, into much shorter representative \"signatures\" of the shingle set, and this allows for much less expensive comparisons between two documents, as we do not need to compute the intersection and union of large sets, and can compare the much shorter signatures instead. Our min-hash works by creating 100 independent hash functions, and having each hash function be applied to each value in the shingle set. \n",
    "\n",
    "For each hash function, the lowest value that was computed across all the hashes is recorded, and the min hash vector for a document is comprised of the 100 \"lowest values\" that each hash function computed. With this, the larger shingle set for each document can be represented by its much smaller 100 value signature vector. The signature vectors for each document are then combined to create the signature matrix of our dataset, which can be used for Locality Snesitive Hashing to be able to identify candidate pairs that might be similar. When min-hashing all of our documents, it takes around 15 seconds to be able to create the signature matrix.\n",
    "\n",
    "### **Locality Sensitive Hashing**\n",
    "\n",
    "The final step in our implemented process is to do locality sensitive hashing onto our signature matrix. Locality sensitive hashing splits up the matrix into \"bands\" of rows, where in each band, the columns of our matrix within that band get hashed into buckets. Once this hashing is done across all bands, we can then identify \"candidate pairs\" as any pair of documents where in at least one band, they were hashed to the same bucket (in other words, any two documents where the signature vectors are identical in at least one section of rows). These candidate pairs are then the pairs that actually get compared to then confirm whether or not they are actually similar. LSH significantly reduces how many similarity comparisons we need to do by filtering out the pairs that do not have an exact signature match in any band, and allows us to be able to sift through similar documents on much larger datasets as a result. \n",
    "\n",
    "The band size plays a large factor in LSH, as a larger number of rows per bands (i.e. less bands) causes for less false positives but more false negatives (as now for a pair to be a candidate pair, they must agree exactly over a larger sequence of rows). As a result of this, we tested a few different configurations of band sizes to see which were better suited to find a good balance between low false-positives, and low false-negatives. The different band sizes also caused some different times for how long the LSH took, as with less bands, because the critertia for being a candidate pair is more strict, there are less candidate pairs identified and thus less comparisons needed. For our full dataset, the time it took for LSH to identify similar pairs was from around 1-3.5 seconds depending on our band size.\n",
    "\n",
    "### **Testing**\n",
    "\n",
    "The overwehlming benefit of min-hashing/LSH is that it allows for much faster similarity comparisons, and narrows the field of pairs to search through significantly, which allows for much faster run times over a brute-force approach of checking the Jaccard Similarity across all pairs of documents. We set up samples of 100, 1000, 10000, and all of our documents (24738), and identified similar pairs using the brute force method and with the min-hashing + LSH process to compare the time it took to complete processing. We also tested on an LSH with 4 bands, 10 bands, and 20 bands (25, 10, and 5 rows per band respectively) to obersve how much decreasing the band size would increase the processing time:\n",
    "\n",
    "|             |  100     |  1000  |  10000  |  All    |\n",
    "|:-----------:|:--------:|:------:|:-------:|:-------:|\n",
    "| **Na√Øve**   | .2 sec   | 5.4 sec| 779 sec | N/A     |\n",
    "| **4 bands** | < .1 sec | .1 sec | 1.6 sec | 16.7 sec|\n",
    "| **10 bands**| < .1 sec | .6 sec | 4.9 sec | 17.5 sec|\n",
    "| **20 bands**| .1 sec   | 1.0 sec| 7.6 sec | 19.2 sec|\n",
    "\n",
    "The Naive Jaccard method works fine on very small datasets, but given the O(n^2) nature of this approach, the computing time becomes very large as the dataset size increases, going from one fifth of a second to search through 100 documents, to almost 13 minutes for a dataset of 10000. With this method, searching through our whole dataset of nearly 25000 documents to find similar pairs is simply not feasible, and so we can see plainly the benefit of Min-hashing + LSH, as we are able to search for similar pairs through our whole dataset in a reasonable timeframe (16-20 sec). We can also see that slight difference mentioned before in the time the Min-Hashing + LSH approach takes depending on the band size, as for smaller bands sizes (more bands) it takes more time during the LSH portion as more pairs get searched through due to the less strict requirements for being identified as a candidate pair. \n",
    "\n",
    "Given that the band size has an impact on how many pairs are identified as being candidates for similarity, we also tested how many candidates pairs were identified with each of these band sizes, and how many false positives each band size ended up generating once the candidate pairs were checked to see whether they were actually similar:\n",
    "\n",
    "|              | Candidate Pairs | Pairs Found  | False Positive % |\n",
    "|:------------:|:---------------:|:------------:|:----------------:|\n",
    "| **4 Bands**  |      12274      |     12274    |        0         |\n",
    "| **10 Bands** |      28982      |     24448    |      15.64       |\n",
    "| **20 Bands** |      56200      |     25443    |      54.80       |\n",
    "\n",
    "With 4 bands, and a very tight restriction on candidate pairs, we noticed that we did not receive any false positives, and that all candidate pairs turned out to meet our similarity threshold of 80 percent. As the number of bands increased and the band size decreased, we noticed a very significant jump in candidate pairs. Having 10 bands of 10 rows gave us well over double the identified candidate pairs, and almost double the amount of actually similar pairs. This did also come up with the apperance of a sizeable amount of false positives, with 15.64 percent of the candidate pairs not actually being similar. Increasing the band number to 20 (5 rows per band) once again increased the number of candidate pairs by around double the amount, however this time the actual similar pairs did not increase nearly as significantly, gaining slightly less than 1000 actual similar pairs. This resulted in a skyrocketed amount of false positives in this band cnfiguration, as over half of the identified candidate pairs did not end up meeting our similarity threshold, and so the small improvement in identified pairs relative to the spike in false positives renders this configuration not optimal for our dataset. \n",
    "\n",
    "### **Conclusions**\n",
    "\n",
    "While brute force Jaccard similarity does identify all pairs that meet a similarity threshold, and can work fine with very small datasets, it proves infeasible quickly. Through our min-hashing + LSH implementation, we can see in our test dataset the impact it has, allowing us to sift through our whole dataset to look for similar pairs of documents, something that was not reasonable with a brute forch approach. We also saw the importance of configuring the band amount in LSH to tailor to what is important when identifying candidate pairs. With our dataset, should ensuring that no identified pair turns out to not be similar is neccessary, than a configuration with 4 bands might work. If making sure that a good amount of the actual similar pairs get identified, with some room for false positives being acceptable, a different configuration of 10 bands might be better suited, and this tuning of the LSH setup also applies outside of our dataset. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-mining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
