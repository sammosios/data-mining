{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset: each line contains space-separated item IDs\n",
    "def load_transactions(path):\n",
    "    transactions = []\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            items = line.strip().split()\n",
    "            if items:\n",
    "                transactions.append(frozenset(items))\n",
    "    return transactions\n",
    "\n",
    "transactions = load_transactions(\"dataset.dat\")\n",
    "print(f\"Loaded {len(transactions)} transactions.\")\n",
    "transactions[:5]  # preview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_support(candidates, transactions):\n",
    "    \"\"\"Return dictionary: candidate_itemset -> support_count\"\"\"\n",
    "    support = defaultdict(int)\n",
    "    for transaction in transactions:\n",
    "        for candidate in candidates:\n",
    "            if candidate.issubset(transaction):\n",
    "                support[candidate] += 1\n",
    "    return support\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_L1(transactions, min_support):\n",
    "    item_counts = defaultdict(int)\n",
    "\n",
    "    for transaction in transactions:\n",
    "        for item in transaction:\n",
    "            item_counts[frozenset([item])] += 1\n",
    "\n",
    "    L1 = {itemset: count for itemset, count in item_counts.items()\n",
    "          if count >= min_support}\n",
    "    \n",
    "    return L1\n",
    "\n",
    "# Example threshold; tune as needed\n",
    "min_support = 1000\n",
    "\n",
    "L1 = generate_L1(transactions, min_support)\n",
    "print(\"Frequent 1-itemsets:\", len(L1))\n",
    "L1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidates(prev_frequent_itemsets, k):\n",
    "    \"\"\"Generate Ck from L(k-1) via self-join and pruning\"\"\"\n",
    "    prev_itemsets = list(prev_frequent_itemsets.keys())\n",
    "    candidates = set()\n",
    "\n",
    "    # Self-join\n",
    "    for i in range(len(prev_itemsets)):\n",
    "        for j in range(i + 1, len(prev_itemsets)):\n",
    "            L1 = list(prev_itemsets[i])\n",
    "            L2 = list(prev_itemsets[j])\n",
    "            L1.sort(); L2.sort()\n",
    "\n",
    "            # If first k-2 items are equal, join them\n",
    "            if L1[:k-2] == L2[:k-2]:\n",
    "                new_candidate = frozenset(set(prev_itemsets[i]) | set(prev_itemsets[j]))\n",
    "                if len(new_candidate) == k:\n",
    "                    \n",
    "                    # Apriori prune:\n",
    "                    # All (k-1)-subsets must be frequent\n",
    "                    all_subsets_frequent = True\n",
    "                    for subset in itertools.combinations(new_candidate, k-1):\n",
    "                        if frozenset(subset) not in prev_frequent_itemsets:\n",
    "                            all_subsets_frequent = False\n",
    "                            break\n",
    "\n",
    "                    if all_subsets_frequent:\n",
    "                        candidates.add(new_candidate)\n",
    "\n",
    "    return candidates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apriori(transactions, min_support):\n",
    "    # Step 1: L1\n",
    "    frequent_itemsets = []\n",
    "    Lk = generate_L1(transactions, min_support)\n",
    "    frequent_itemsets.append(Lk)\n",
    "    \n",
    "    k = 2\n",
    "\n",
    "    while True:\n",
    "        print(f\"Generating candidates for k = {k}\")\n",
    "\n",
    "        Ck = generate_candidates(Lk, k)\n",
    "        if not Ck:\n",
    "            break\n",
    "\n",
    "        support_counts = count_support(Ck, transactions)\n",
    "\n",
    "        # Filter by support threshold\n",
    "        Lk = {itemset: count for itemset, count in support_counts.items()\n",
    "              if count >= min_support}\n",
    "\n",
    "        if not Lk:\n",
    "            break\n",
    "\n",
    "        frequent_itemsets.append(Lk)\n",
    "        k += 1\n",
    "\n",
    "    return frequent_itemsets\n",
    "\n",
    "frequent_itemsets = apriori(transactions, min_support)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = sum(len(level) for level in frequent_itemsets)\n",
    "\n",
    "print(f\"Total frequent itemsets: {total}\\n\")\n",
    "\n",
    "for i, Lk in enumerate(frequent_itemsets, start=1):\n",
    "    print(f\"Level {i} — {len(Lk)} itemsets\")\n",
    "    for itemset, support in Lk.items():\n",
    "        print(f\"  {set(itemset)}  → support {support}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate association rules from frequent itemsets\n",
    "def generate_association_rules(frequent_itemsets, min_confidence):\n",
    "    rules = []\n",
    "\n",
    "    # Flatten frequent itemsets into: itemset -> support\n",
    "    all_frequents = {}\n",
    "    for level in frequent_itemsets:\n",
    "        all_frequents.update(level)\n",
    "\n",
    "    for itemset, itemset_support in all_frequents.items():\n",
    "        if len(itemset) < 2:\n",
    "            continue  # can't split 1-itemset into a rule\n",
    "\n",
    "        items = list(itemset)\n",
    "\n",
    "        # Generate all non-empty proper subsets X ⊂ itemset\n",
    "        for r in range(1, len(items)):\n",
    "            for X in itertools.combinations(items, r):\n",
    "                X = frozenset(X)\n",
    "                Y = itemset - X\n",
    "\n",
    "                if X in all_frequents:\n",
    "                    confidence = itemset_support / all_frequents[X]\n",
    "\n",
    "                    if confidence >= min_confidence:\n",
    "                        rule = {\n",
    "                            \"X\": X,\n",
    "                            \"Y\": Y,\n",
    "                            \"support\": itemset_support,\n",
    "                            \"confidence\": confidence\n",
    "                        }\n",
    "                        rules.append(rule)\n",
    "    return rules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_confidence = 0.3\n",
    "\n",
    "rules = generate_association_rules(frequent_itemsets, min_confidence)\n",
    "\n",
    "print(f\"Generated {len(rules)} association rules.\")\n",
    "\n",
    "rules_sorted = sorted(\n",
    "    rules,\n",
    "    key=lambda r: (r['confidence'], r['support']),\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "for rule in rules_sorted[:20]:\n",
    "    print(\n",
    "        f\"{set(rule['X'])} -> {set(rule['Y'])} \"\n",
    "        f\"(conf={rule['confidence']:.3f}, support={rule['support']})\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## **Report**\n",
    "\n",
    "### **Instructions**\n",
    "\n",
    "The dataset was downloaded from the Canvas assignment page, and copied next to the notebook as ```dataset.dat```.\n",
    "\n",
    "### **Introduction**\n",
    "\n",
    "In this seminar, we implemented the A-Priori algorithm to identify frequent itemsets that occur in a set of transactions that have support over a given threshold. The support for a particular itemset is simply how many transactions in the dataset contain the itemset. Once those itemsets have been identified, we then generate association rules from those frequent itemsets to determine which rules have a high confidence. This process has applications in many domains, such as for vendors to determine whether certain purchases may incline a customer to also include other items in their purchase, or in pharmaceutical cases where side effects that come along with certain combinations of prescriptions can be identified.\n",
    "\n",
    "### **Identifying Frequent Itemsets**\n",
    "\n",
    "In order for an itemset to be frequent, it must appear over a certain support threshold fraction of the transactions in the dataset. However, there are many itemsets to check out, especially once you get to itemsets with longer lengths. If you take a generous asumption of a transactions set that has 3000 unique items, that will have almost 500,000 unique pairs, and raise that to itemsets of length 3 and you get over 150 million unique triplets. Given the exponential size of possible itemsets, a simple method of naively checking how many transactions each itemset is a part of quickly becomes impossible. Due to this, we need a different method of identifying frequent itemsets that allows us to identify a much smaller set of \"likely\" itemsets that we can check through to identify frequent itemsets.\n",
    "\n",
    "This is where the A-Priori Algorithm comes into play. It is able to prune through and filter out many possible itemsets as not being frequent through the principle that a frequent itemset can only be that way if every subset is also a frequent itemset. For any itemset, adding another element to that itemset can only lower the ratio of transactions that it appears in, and so once an itemset becomes infrequent, any add-on to that base itemset will also remain infrequent. A-Priori utilizes this property, as when building the possible itemsets of length k that could be frequent, it only builds these candidate itemsets through combining the itemsets of length k-1 that were frequent, with frequent itemsets of length 1. This results in a far lower number of itemsets at each length that need to be checked. Instead of checking all possible pairs of, for instance, length 3 itemsets, it can build a smaller candidate pool by only checking the length 3 itemsets that result from a frequent length 2 itemset and a frequent singelton (length 1 itemset).\n",
    "\n",
    "We first generate the set L1, which is the set of all singleton itemsets that do meet the support threshold. From L1, we can then create the candidate list C2 for itemsets of length 2 from frequent itemsets in L1. Once C2 is created, the itemsets are then checked to see if they match the support requirement, and then the process repeats iteratively for C3, C4, and so on, continously building new candidate itemsets from the previous L(k-1). Doing this allows us to be able to identify frequent itemsets of larger degrees, whereas trying to check through all possible itemsets proves infeasible almost immediately. \n",
    "\n",
    "From testing on our dataset, at a supportability of .6 we notice it can take around 5-7 minutes to be able to identify all frequent itemsets, as despite pruning out infeasible candidate pairs that don't need to actively be considered throughout the process, there are still many candidate itemsets that can be created from our previous levels of frequent itemsets. From our dataset, the vast majority of the time is spent on creating itemsets of length 2, as with our support of .6, we have 375 length 1 itemsets identified, which comes out to around 70,000 different length 2 itemsets to check. After this however, our implementation is very quick at runtime to check candidate itemsets of higher lengths. This is due to the creation of k-size candidate sets being based off smaller length previous frequent itemsets, as this allows us to keep making our candidate sets smaller as our searched itemset size increases. As a result, this algorithm is applicable even on sets that can have larger size frequent itemsets, as each successive step has to go through less possible candidates.\n",
    "\n",
    "The support threshold also is a large factor in how long the a-priori algorithm takes, as setting a higher support threshold results in less starting length 1 itemsets that are identified as frequent, and so the algorithm will have less candidates at the C2 stage to construct. Increasing our support thereshold from 1000 to 2000 causes the initially identified frequent 1 pair itemsets to only be 155, and this decreases our C2 size drastically, and so as a result all frequent itemsets get identified in around 1 minute. Increasing supportability does impact the future association rule generation though, as less identified freuent itemsets allowes for less rules to be created and checked, when raising our supportbaility to 2000 for instance we now don't get any frequent itemsets of length > 1, and so as a result we cannot form any rules that meet our set supportability. As a result, the selected support threshold should depend on several factors: how large the dataset is and how feasible it is to run the algotihm based on the initial size of your C2 set, what level of supportability is considered significant in the context, and whether having rules with high confidence is paramount, rather than those rules having a high supportability being the important factor.\n",
    "\n",
    "### **Generating Association Rules**\n",
    "\n",
    "From frequent item sets, we can then generate association rules from them. The way that we generate these association rules is that for every itemset, we look at every proper subset P of that itemset (as a result itemset should be > 1 length), and we generate the rule that P -> {itemset}/P, and we check the confidence to see whether it meets our set confidence level. We guarantee that the association rules meet the support criteria, as we generate all rules from frequent itemsets, and so as a result for a generated X -> Y rule, X U Y is going to be some frequent itemset, which by definition meets our support requirement. \n",
    "\n",
    "From testing on our dataset, we notice that the confidence level has a very high impact on the identified association rules that meet the confidence criteria. When testing with a supportability of .60 for identifying frequent itemsets, we were able to get 9 itemsets of length 2 and 1 itemset of length 3, which gives us 24 total possible association rules. As a result, it does not take any considerable time to generate association rules given the low number of total possible rules, and this is as a result from being able to succesfully prune out infrequent itemsets previously, allowing us to build association rules that have a support above our accepted threshold. From testing with different confidence interval, the percentage of rules that met the confidence criteria varied heavily with the confidence threshold: \n",
    "\n",
    "| Confidence Threshold | # Of Generated Rules | Percentage of Rules Meeting Confidence |\n",
    "|----------------------|----------------------|----------------------------------------|\n",
    "|         0.9          |           2          |                 0.083                  |\n",
    "|         0.8          |           3          |                 0.125                  |\n",
    "|         0.6          |           5          |                 0.208                  |\n",
    "|         0.5          |           7          |                 0.291                  |\n",
    "|         0.3          |           13         |                 0.541                  |\n",
    "\n",
    "From this we can see that the percentage of rules that meet a criteria steadily increases as confidence decreases, which shows that the confidence of a rule is not weighed heavily towards being very high or very low, and is more evenly distributed from 0 to 1. Due to confidence not being skewed towards very high or very low values, slightly low confidence readings of .3-.6 may not be sufficient to be able to follow, as they are quite normal in our distribution of confidence levels and may not suggest any consistent pattern or trend. The selected confidence that is acceptable is dependent on the environment as well, for something like retail where it may not be too costly to put certain items close together and more liberties can be taken to increase sales, lower confidence and less consistency in a pattern might be alright and enough to still group certain items together in a section of the store. However for other applications where the rules that are trusted to be consistent need to be very consistent, higher confidence values would need to be chosen to ensure loose rules are not followed as a pattern.\n",
    "\n",
    "### **Conclusion**\n",
    "\n",
    "This was an insighftul experiment into how frequent itemsets can be reasonably obtained from a dataset, and how those itemsets can be used to generate association rules that can form observations about our data. A simple analysis into the amount of possible itemsets quickly shows that it is imperative to find algorithms to be able to progressively tune out more and more possibilities as being infrequent from the beginning, and implementing A-Priori showed us a method of doing that, and helped with understanding the underlying set logic behind how it is able to go about that. Generating association rules also helped us to visualize how real world observations can then be made after identifying frequent itemsets, and how those observations will innately have high support due to the nature of how those rules were formed from identified frequent itemsets.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-mining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
