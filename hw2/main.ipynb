{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset: each line contains space-separated item IDs\n",
    "def load_transactions(path):\n",
    "    transactions = []\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            items = line.strip().split()\n",
    "            if items:\n",
    "                transactions.append(frozenset(items))\n",
    "    return transactions\n",
    "\n",
    "transactions = load_transactions(\"dataset.dat\")\n",
    "print(f\"Loaded {len(transactions)} transactions.\")\n",
    "transactions[:5]  # preview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_support(candidates, transactions):\n",
    "    \"\"\"Return dictionary: candidate_itemset -> support_count\"\"\"\n",
    "    support = defaultdict(int)\n",
    "    for transaction in transactions:\n",
    "        for candidate in candidates:\n",
    "            if candidate.issubset(transaction):\n",
    "                support[candidate] += 1\n",
    "    return support\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_L1(transactions, min_support):\n",
    "    item_counts = defaultdict(int)\n",
    "\n",
    "    for transaction in transactions:\n",
    "        for item in transaction:\n",
    "            item_counts[frozenset([item])] += 1\n",
    "\n",
    "    L1 = {itemset: count for itemset, count in item_counts.items()\n",
    "          if count >= min_support}\n",
    "    \n",
    "    return L1\n",
    "\n",
    "# Example threshold; tune as needed\n",
    "min_support = 1000\n",
    "\n",
    "L1 = generate_L1(transactions, min_support)\n",
    "print(\"Frequent 1-itemsets:\", len(L1))\n",
    "L1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidates(prev_frequent_itemsets, k):\n",
    "    \"\"\"Generate Ck from L(k-1) via self-join and pruning\"\"\"\n",
    "    prev_itemsets = list(prev_frequent_itemsets.keys())\n",
    "    candidates = set()\n",
    "\n",
    "    # Self-join\n",
    "    for i in range(len(prev_itemsets)):\n",
    "        for j in range(i + 1, len(prev_itemsets)):\n",
    "            L1 = list(prev_itemsets[i])\n",
    "            L2 = list(prev_itemsets[j])\n",
    "            L1.sort(); L2.sort()\n",
    "\n",
    "            # If first k-2 items are equal, join them\n",
    "            if L1[:k-2] == L2[:k-2]:\n",
    "                new_candidate = frozenset(set(prev_itemsets[i]) | set(prev_itemsets[j]))\n",
    "                if len(new_candidate) == k:\n",
    "                    \n",
    "                    # Apriori prune:\n",
    "                    # All (k-1)-subsets must be frequent\n",
    "                    all_subsets_frequent = True\n",
    "                    for subset in itertools.combinations(new_candidate, k-1):\n",
    "                        if frozenset(subset) not in prev_frequent_itemsets:\n",
    "                            all_subsets_frequent = False\n",
    "                            break\n",
    "\n",
    "                    if all_subsets_frequent:\n",
    "                        candidates.add(new_candidate)\n",
    "\n",
    "    return candidates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apriori(transactions, min_support):\n",
    "    # Step 1: L1\n",
    "    frequent_itemsets = []\n",
    "    Lk = generate_L1(transactions, min_support)\n",
    "    frequent_itemsets.append(Lk)\n",
    "    \n",
    "    k = 2\n",
    "\n",
    "    while True:\n",
    "        print(f\"Generating candidates for k = {k}\")\n",
    "\n",
    "        Ck = generate_candidates(Lk, k)\n",
    "        if not Ck:\n",
    "            break\n",
    "\n",
    "        support_counts = count_support(Ck, transactions)\n",
    "\n",
    "        # Filter by support threshold\n",
    "        Lk = {itemset: count for itemset, count in support_counts.items()\n",
    "              if count >= min_support}\n",
    "\n",
    "        if not Lk:\n",
    "            break\n",
    "\n",
    "        frequent_itemsets.append(Lk)\n",
    "        k += 1\n",
    "\n",
    "    return frequent_itemsets\n",
    "\n",
    "frequent_itemsets = apriori(transactions, min_support)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = sum(len(level) for level in frequent_itemsets)\n",
    "\n",
    "print(f\"Total frequent itemsets: {total}\\n\")\n",
    "\n",
    "for i, Lk in enumerate(frequent_itemsets, start=1):\n",
    "    print(f\"Level {i} — {len(Lk)} itemsets\")\n",
    "    for itemset, support in Lk.items():\n",
    "        print(f\"  {set(itemset)}  → support {support}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate association rules from frequent itemsets\n",
    "def generate_association_rules(frequent_itemsets, min_confidence):\n",
    "    rules = []\n",
    "\n",
    "    # Flatten frequent itemsets into: itemset -> support\n",
    "    all_frequents = {}\n",
    "    for level in frequent_itemsets:\n",
    "        all_frequents.update(level)\n",
    "\n",
    "    for itemset, itemset_support in all_frequents.items():\n",
    "        if len(itemset) < 2:\n",
    "            continue  # can't split 1-itemset into a rule\n",
    "\n",
    "        items = list(itemset)\n",
    "\n",
    "        # Generate all non-empty proper subsets X ⊂ itemset\n",
    "        for r in range(1, len(items)):\n",
    "            for X in itertools.combinations(items, r):\n",
    "                X = frozenset(X)\n",
    "                Y = itemset - X\n",
    "\n",
    "                if X in all_frequents:\n",
    "                    confidence = itemset_support / all_frequents[X]\n",
    "\n",
    "                    if confidence >= min_confidence:\n",
    "                        rule = {\n",
    "                            \"X\": X,\n",
    "                            \"Y\": Y,\n",
    "                            \"support\": itemset_support,\n",
    "                            \"confidence\": confidence\n",
    "                        }\n",
    "                        rules.append(rule)\n",
    "    return rules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_confidence = 0.6\n",
    "\n",
    "rules = generate_association_rules(frequent_itemsets, min_confidence)\n",
    "\n",
    "print(f\"Generated {len(rules)} association rules.\")\n",
    "\n",
    "rules_sorted = sorted(\n",
    "    rules,\n",
    "    key=lambda r: (r['confidence'], r['support']),\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "for rule in rules_sorted[:20]:\n",
    "    print(\n",
    "        f\"{set(rule['X'])} -> {set(rule['Y'])} \"\n",
    "        f\"(conf={rule['confidence']:.3f}, support={rule['support']})\"\n",
    "    )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-mining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
