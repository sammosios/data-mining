{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100000 transactions.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[frozenset({'164',\n",
       "            '240',\n",
       "            '25',\n",
       "            '274',\n",
       "            '328',\n",
       "            '368',\n",
       "            '448',\n",
       "            '52',\n",
       "            '538',\n",
       "            '561',\n",
       "            '630',\n",
       "            '687',\n",
       "            '730',\n",
       "            '775',\n",
       "            '825',\n",
       "            '834'}),\n",
       " frozenset({'120',\n",
       "            '124',\n",
       "            '205',\n",
       "            '39',\n",
       "            '401',\n",
       "            '581',\n",
       "            '704',\n",
       "            '814',\n",
       "            '825',\n",
       "            '834'}),\n",
       " frozenset({'249', '35', '674', '712', '733', '759', '854', '950'}),\n",
       " frozenset({'39',\n",
       "            '422',\n",
       "            '449',\n",
       "            '704',\n",
       "            '825',\n",
       "            '857',\n",
       "            '895',\n",
       "            '937',\n",
       "            '954',\n",
       "            '964'}),\n",
       " frozenset({'15',\n",
       "            '229',\n",
       "            '262',\n",
       "            '283',\n",
       "            '294',\n",
       "            '352',\n",
       "            '381',\n",
       "            '708',\n",
       "            '738',\n",
       "            '766',\n",
       "            '853',\n",
       "            '883',\n",
       "            '966',\n",
       "            '978'})]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset: each line contains space-separated item IDs\n",
    "def load_transactions(path):\n",
    "    transactions = []\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            items = line.strip().split()\n",
    "            if items:\n",
    "                transactions.append(frozenset(items))\n",
    "    return transactions\n",
    "\n",
    "transactions = load_transactions(\"dataset.dat\")\n",
    "print(f\"Loaded {len(transactions)} transactions.\")\n",
    "transactions[:5]  # preview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_support(candidates, transactions):\n",
    "    \"\"\"Return dictionary: candidate_itemset -> support_count\"\"\"\n",
    "    support = defaultdict(int)\n",
    "    for transaction in transactions:\n",
    "        for candidate in candidates:\n",
    "            if candidate.issubset(transaction):\n",
    "                support[candidate] += 1\n",
    "    return support\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequent 1-itemsets: 155\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{frozenset({'368'}): 7828,\n",
       " frozenset({'274'}): 2628,\n",
       " frozenset({'561'}): 2783,\n",
       " frozenset({'538'}): 3982,\n",
       " frozenset({'775'}): 3771,\n",
       " frozenset({'825'}): 3085,\n",
       " frozenset({'205'}): 3605,\n",
       " frozenset({'401'}): 3667,\n",
       " frozenset({'581'}): 2943,\n",
       " frozenset({'120'}): 4973,\n",
       " frozenset({'39'}): 4258,\n",
       " frozenset({'854'}): 2847,\n",
       " frozenset({'674'}): 2527,\n",
       " frozenset({'895'}): 3385,\n",
       " frozenset({'937'}): 4681,\n",
       " frozenset({'766'}): 6265,\n",
       " frozenset({'738'}): 2129,\n",
       " frozenset({'229'}): 2281,\n",
       " frozenset({'883'}): 4902,\n",
       " frozenset({'381'}): 2959,\n",
       " frozenset({'966'}): 3921,\n",
       " frozenset({'283'}): 4082,\n",
       " frozenset({'620'}): 2100,\n",
       " frozenset({'798'}): 3103,\n",
       " frozenset({'569'}): 2835,\n",
       " frozenset({'782'}): 2767,\n",
       " frozenset({'529'}): 7057,\n",
       " frozenset({'682'}): 4132,\n",
       " frozenset({'350'}): 3069,\n",
       " frozenset({'947'}): 3690,\n",
       " frozenset({'970'}): 2086,\n",
       " frozenset({'809'}): 2163,\n",
       " frozenset({'390'}): 2685,\n",
       " frozenset({'280'}): 2108,\n",
       " frozenset({'279'}): 3014,\n",
       " frozenset({'675'}): 2976,\n",
       " frozenset({'192'}): 2004,\n",
       " frozenset({'914'}): 4037,\n",
       " frozenset({'720'}): 3864,\n",
       " frozenset({'71'}): 3507,\n",
       " frozenset({'597'}): 2883,\n",
       " frozenset({'276'}): 2479,\n",
       " frozenset({'878'}): 2047,\n",
       " frozenset({'217'}): 5375,\n",
       " frozenset({'183'}): 3883,\n",
       " frozenset({'653'}): 2634,\n",
       " frozenset({'960'}): 2732,\n",
       " frozenset({'177'}): 4629,\n",
       " frozenset({'161'}): 2320,\n",
       " frozenset({'175'}): 2791,\n",
       " frozenset({'571'}): 2902,\n",
       " frozenset({'795'}): 3361,\n",
       " frozenset({'862'}): 3649,\n",
       " frozenset({'392'}): 2420,\n",
       " frozenset({'78'}): 2471,\n",
       " frozenset({'27'}): 2165,\n",
       " frozenset({'921'}): 2425,\n",
       " frozenset({'778'}): 2514,\n",
       " frozenset({'411'}): 2047,\n",
       " frozenset({'579'}): 2164,\n",
       " frozenset({'803'}): 2237,\n",
       " frozenset({'888'}): 3686,\n",
       " frozenset({'944'}): 2794,\n",
       " frozenset({'614'}): 3134,\n",
       " frozenset({'523'}): 2244,\n",
       " frozenset({'204'}): 2174,\n",
       " frozenset({'334'}): 2146,\n",
       " frozenset({'874'}): 2237,\n",
       " frozenset({'480'}): 2309,\n",
       " frozenset({'70'}): 2411,\n",
       " frozenset({'151'}): 2611,\n",
       " frozenset({'419'}): 5057,\n",
       " frozenset({'73'}): 2179,\n",
       " frozenset({'722'}): 5845,\n",
       " frozenset({'918'}): 3012,\n",
       " frozenset({'844'}): 2814,\n",
       " frozenset({'788'}): 2386,\n",
       " frozenset({'789'}): 4309,\n",
       " frozenset({'526'}): 2793,\n",
       " frozenset({'774'}): 2046,\n",
       " frozenset({'116'}): 2193,\n",
       " frozenset({'541'}): 3735,\n",
       " frozenset({'487'}): 3135,\n",
       " frozenset({'631'}): 2793,\n",
       " frozenset({'638'}): 2288,\n",
       " frozenset({'471'}): 2894,\n",
       " frozenset({'780'}): 2306,\n",
       " frozenset({'956'}): 3626,\n",
       " frozenset({'242'}): 2325,\n",
       " frozenset({'758'}): 2860,\n",
       " frozenset({'885'}): 3043,\n",
       " frozenset({'676'}): 2717,\n",
       " frozenset({'145'}): 4559,\n",
       " frozenset({'617'}): 2614,\n",
       " frozenset({'522'}): 2725,\n",
       " frozenset({'354'}): 5835,\n",
       " frozenset({'12'}): 3415,\n",
       " frozenset({'296'}): 2210,\n",
       " frozenset({'684'}): 5408,\n",
       " frozenset({'548'}): 2843,\n",
       " frozenset({'477'}): 2462,\n",
       " frozenset({'829'}): 6810,\n",
       " frozenset({'210'}): 2009,\n",
       " frozenset({'346'}): 3470,\n",
       " frozenset({'460'}): 4438,\n",
       " frozenset({'919'}): 3710,\n",
       " frozenset({'744'}): 2177,\n",
       " frozenset({'196'}): 2096,\n",
       " frozenset({'494'}): 5102,\n",
       " frozenset({'489'}): 3420,\n",
       " frozenset({'362'}): 4388,\n",
       " frozenset({'472'}): 2125,\n",
       " frozenset({'832'}): 2062,\n",
       " frozenset({'871'}): 2810,\n",
       " frozenset({'72'}): 2852,\n",
       " frozenset({'132'}): 2641,\n",
       " frozenset({'21'}): 2666,\n",
       " frozenset({'32'}): 4248,\n",
       " frozenset({'54'}): 2595,\n",
       " frozenset({'239'}): 2742,\n",
       " frozenset({'48'}): 2472,\n",
       " frozenset({'285'}): 2600,\n",
       " frozenset({'140'}): 2687,\n",
       " frozenset({'387'}): 2089,\n",
       " frozenset({'112'}): 2680,\n",
       " frozenset({'606'}): 2668,\n",
       " frozenset({'93'}): 2777,\n",
       " frozenset({'236'}): 2618,\n",
       " frozenset({'593'}): 2601,\n",
       " frozenset({'69'}): 2370,\n",
       " frozenset({'797'}): 2684,\n",
       " frozenset({'6'}): 2149,\n",
       " frozenset({'509'}): 3044,\n",
       " frozenset({'793'}): 3063,\n",
       " frozenset({'598'}): 3219,\n",
       " frozenset({'470'}): 4137,\n",
       " frozenset({'373'}): 2007,\n",
       " frozenset({'349'}): 2041,\n",
       " frozenset({'8'}): 3090,\n",
       " frozenset({'413'}): 2637,\n",
       " frozenset({'692'}): 4993,\n",
       " frozenset({'694'}): 2847,\n",
       " frozenset({'57'}): 2743,\n",
       " frozenset({'752'}): 2578,\n",
       " frozenset({'998'}): 2713,\n",
       " frozenset({'438'}): 4511,\n",
       " frozenset({'75'}): 3151,\n",
       " frozenset({'38'}): 2402,\n",
       " frozenset({'663'}): 2354,\n",
       " frozenset({'886'}): 3053,\n",
       " frozenset({'510'}): 3281,\n",
       " frozenset({'826'}): 2022,\n",
       " frozenset({'661'}): 2693,\n",
       " frozenset({'634'}): 2492,\n",
       " frozenset({'450'}): 2082}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_L1(transactions, min_support):\n",
    "    item_counts = defaultdict(int)\n",
    "\n",
    "    for transaction in transactions:\n",
    "        for item in transaction:\n",
    "            item_counts[frozenset([item])] += 1\n",
    "\n",
    "    L1 = {itemset: count for itemset, count in item_counts.items()\n",
    "          if count >= min_support}\n",
    "    \n",
    "    return L1\n",
    "\n",
    "# Example threshold; tune as needed\n",
    "min_support = 1000\n",
    "\n",
    "L1 = generate_L1(transactions, min_support)\n",
    "print(\"Frequent 1-itemsets:\", len(L1))\n",
    "L1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidates(prev_frequent_itemsets, k):\n",
    "    \"\"\"Generate Ck from L(k-1) via self-join and pruning\"\"\"\n",
    "    prev_itemsets = list(prev_frequent_itemsets.keys())\n",
    "    candidates = set()\n",
    "\n",
    "    # Self-join\n",
    "    for i in range(len(prev_itemsets)):\n",
    "        for j in range(i + 1, len(prev_itemsets)):\n",
    "            L1 = list(prev_itemsets[i])\n",
    "            L2 = list(prev_itemsets[j])\n",
    "            L1.sort(); L2.sort()\n",
    "\n",
    "            # If first k-2 items are equal, join them\n",
    "            if L1[:k-2] == L2[:k-2]:\n",
    "                new_candidate = frozenset(set(prev_itemsets[i]) | set(prev_itemsets[j]))\n",
    "                if len(new_candidate) == k:\n",
    "                    \n",
    "                    # Apriori prune:\n",
    "                    # All (k-1)-subsets must be frequent\n",
    "                    all_subsets_frequent = True\n",
    "                    for subset in itertools.combinations(new_candidate, k-1):\n",
    "                        if frozenset(subset) not in prev_frequent_itemsets:\n",
    "                            all_subsets_frequent = False\n",
    "                            break\n",
    "\n",
    "                    if all_subsets_frequent:\n",
    "                        candidates.add(new_candidate)\n",
    "\n",
    "    return candidates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating candidates for k = 2\n"
     ]
    }
   ],
   "source": [
    "def apriori(transactions, min_support):\n",
    "    # Step 1: L1\n",
    "    frequent_itemsets = []\n",
    "    Lk = generate_L1(transactions, min_support)\n",
    "    frequent_itemsets.append(Lk)\n",
    "    \n",
    "    k = 2\n",
    "\n",
    "    while True:\n",
    "        print(f\"Generating candidates for k = {k}\")\n",
    "\n",
    "        Ck = generate_candidates(Lk, k)\n",
    "        if not Ck:\n",
    "            break\n",
    "\n",
    "        support_counts = count_support(Ck, transactions)\n",
    "\n",
    "        # Filter by support threshold\n",
    "        Lk = {itemset: count for itemset, count in support_counts.items()\n",
    "              if count >= min_support}\n",
    "\n",
    "        if not Lk:\n",
    "            break\n",
    "\n",
    "        frequent_itemsets.append(Lk)\n",
    "        k += 1\n",
    "\n",
    "    return frequent_itemsets\n",
    "\n",
    "frequent_itemsets = apriori(transactions, min_support)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total frequent itemsets: 155\n",
      "\n",
      "Level 1 — 155 itemsets\n",
      "  {'368'}  → support 7828\n",
      "  {'274'}  → support 2628\n",
      "  {'561'}  → support 2783\n",
      "  {'538'}  → support 3982\n",
      "  {'775'}  → support 3771\n",
      "  {'825'}  → support 3085\n",
      "  {'205'}  → support 3605\n",
      "  {'401'}  → support 3667\n",
      "  {'581'}  → support 2943\n",
      "  {'120'}  → support 4973\n",
      "  {'39'}  → support 4258\n",
      "  {'854'}  → support 2847\n",
      "  {'674'}  → support 2527\n",
      "  {'895'}  → support 3385\n",
      "  {'937'}  → support 4681\n",
      "  {'766'}  → support 6265\n",
      "  {'738'}  → support 2129\n",
      "  {'229'}  → support 2281\n",
      "  {'883'}  → support 4902\n",
      "  {'381'}  → support 2959\n",
      "  {'966'}  → support 3921\n",
      "  {'283'}  → support 4082\n",
      "  {'620'}  → support 2100\n",
      "  {'798'}  → support 3103\n",
      "  {'569'}  → support 2835\n",
      "  {'782'}  → support 2767\n",
      "  {'529'}  → support 7057\n",
      "  {'682'}  → support 4132\n",
      "  {'350'}  → support 3069\n",
      "  {'947'}  → support 3690\n",
      "  {'970'}  → support 2086\n",
      "  {'809'}  → support 2163\n",
      "  {'390'}  → support 2685\n",
      "  {'280'}  → support 2108\n",
      "  {'279'}  → support 3014\n",
      "  {'675'}  → support 2976\n",
      "  {'192'}  → support 2004\n",
      "  {'914'}  → support 4037\n",
      "  {'720'}  → support 3864\n",
      "  {'71'}  → support 3507\n",
      "  {'597'}  → support 2883\n",
      "  {'276'}  → support 2479\n",
      "  {'878'}  → support 2047\n",
      "  {'217'}  → support 5375\n",
      "  {'183'}  → support 3883\n",
      "  {'653'}  → support 2634\n",
      "  {'960'}  → support 2732\n",
      "  {'177'}  → support 4629\n",
      "  {'161'}  → support 2320\n",
      "  {'175'}  → support 2791\n",
      "  {'571'}  → support 2902\n",
      "  {'795'}  → support 3361\n",
      "  {'862'}  → support 3649\n",
      "  {'392'}  → support 2420\n",
      "  {'78'}  → support 2471\n",
      "  {'27'}  → support 2165\n",
      "  {'921'}  → support 2425\n",
      "  {'778'}  → support 2514\n",
      "  {'411'}  → support 2047\n",
      "  {'579'}  → support 2164\n",
      "  {'803'}  → support 2237\n",
      "  {'888'}  → support 3686\n",
      "  {'944'}  → support 2794\n",
      "  {'614'}  → support 3134\n",
      "  {'523'}  → support 2244\n",
      "  {'204'}  → support 2174\n",
      "  {'334'}  → support 2146\n",
      "  {'874'}  → support 2237\n",
      "  {'480'}  → support 2309\n",
      "  {'70'}  → support 2411\n",
      "  {'151'}  → support 2611\n",
      "  {'419'}  → support 5057\n",
      "  {'73'}  → support 2179\n",
      "  {'722'}  → support 5845\n",
      "  {'918'}  → support 3012\n",
      "  {'844'}  → support 2814\n",
      "  {'788'}  → support 2386\n",
      "  {'789'}  → support 4309\n",
      "  {'526'}  → support 2793\n",
      "  {'774'}  → support 2046\n",
      "  {'116'}  → support 2193\n",
      "  {'541'}  → support 3735\n",
      "  {'487'}  → support 3135\n",
      "  {'631'}  → support 2793\n",
      "  {'638'}  → support 2288\n",
      "  {'471'}  → support 2894\n",
      "  {'780'}  → support 2306\n",
      "  {'956'}  → support 3626\n",
      "  {'242'}  → support 2325\n",
      "  {'758'}  → support 2860\n",
      "  {'885'}  → support 3043\n",
      "  {'676'}  → support 2717\n",
      "  {'145'}  → support 4559\n",
      "  {'617'}  → support 2614\n",
      "  {'522'}  → support 2725\n",
      "  {'354'}  → support 5835\n",
      "  {'12'}  → support 3415\n",
      "  {'296'}  → support 2210\n",
      "  {'684'}  → support 5408\n",
      "  {'548'}  → support 2843\n",
      "  {'477'}  → support 2462\n",
      "  {'829'}  → support 6810\n",
      "  {'210'}  → support 2009\n",
      "  {'346'}  → support 3470\n",
      "  {'460'}  → support 4438\n",
      "  {'919'}  → support 3710\n",
      "  {'744'}  → support 2177\n",
      "  {'196'}  → support 2096\n",
      "  {'494'}  → support 5102\n",
      "  {'489'}  → support 3420\n",
      "  {'362'}  → support 4388\n",
      "  {'472'}  → support 2125\n",
      "  {'832'}  → support 2062\n",
      "  {'871'}  → support 2810\n",
      "  {'72'}  → support 2852\n",
      "  {'132'}  → support 2641\n",
      "  {'21'}  → support 2666\n",
      "  {'32'}  → support 4248\n",
      "  {'54'}  → support 2595\n",
      "  {'239'}  → support 2742\n",
      "  {'48'}  → support 2472\n",
      "  {'285'}  → support 2600\n",
      "  {'140'}  → support 2687\n",
      "  {'387'}  → support 2089\n",
      "  {'112'}  → support 2680\n",
      "  {'606'}  → support 2668\n",
      "  {'93'}  → support 2777\n",
      "  {'236'}  → support 2618\n",
      "  {'593'}  → support 2601\n",
      "  {'69'}  → support 2370\n",
      "  {'797'}  → support 2684\n",
      "  {'6'}  → support 2149\n",
      "  {'509'}  → support 3044\n",
      "  {'793'}  → support 3063\n",
      "  {'598'}  → support 3219\n",
      "  {'470'}  → support 4137\n",
      "  {'373'}  → support 2007\n",
      "  {'349'}  → support 2041\n",
      "  {'8'}  → support 3090\n",
      "  {'413'}  → support 2637\n",
      "  {'692'}  → support 4993\n",
      "  {'694'}  → support 2847\n",
      "  {'57'}  → support 2743\n",
      "  {'752'}  → support 2578\n",
      "  {'998'}  → support 2713\n",
      "  {'438'}  → support 4511\n",
      "  {'75'}  → support 3151\n",
      "  {'38'}  → support 2402\n",
      "  {'663'}  → support 2354\n",
      "  {'886'}  → support 3053\n",
      "  {'510'}  → support 3281\n",
      "  {'826'}  → support 2022\n",
      "  {'661'}  → support 2693\n",
      "  {'634'}  → support 2492\n",
      "  {'450'}  → support 2082\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total = sum(len(level) for level in frequent_itemsets)\n",
    "\n",
    "print(f\"Total frequent itemsets: {total}\\n\")\n",
    "\n",
    "for i, Lk in enumerate(frequent_itemsets, start=1):\n",
    "    print(f\"Level {i} — {len(Lk)} itemsets\")\n",
    "    for itemset, support in Lk.items():\n",
    "        print(f\"  {set(itemset)}  → support {support}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate association rules from frequent itemsets\n",
    "def generate_association_rules(frequent_itemsets, min_confidence):\n",
    "    rules = []\n",
    "\n",
    "    # Flatten frequent itemsets into: itemset -> support\n",
    "    all_frequents = {}\n",
    "    for level in frequent_itemsets:\n",
    "        all_frequents.update(level)\n",
    "\n",
    "    for itemset, itemset_support in all_frequents.items():\n",
    "        if len(itemset) < 2:\n",
    "            continue  # can't split 1-itemset into a rule\n",
    "\n",
    "        items = list(itemset)\n",
    "\n",
    "        # Generate all non-empty proper subsets X ⊂ itemset\n",
    "        for r in range(1, len(items)):\n",
    "            for X in itertools.combinations(items, r):\n",
    "                X = frozenset(X)\n",
    "                Y = itemset - X\n",
    "\n",
    "                if X in all_frequents:\n",
    "                    confidence = itemset_support / all_frequents[X]\n",
    "\n",
    "                    if confidence >= min_confidence:\n",
    "                        rule = {\n",
    "                            \"X\": X,\n",
    "                            \"Y\": Y,\n",
    "                            \"support\": itemset_support,\n",
    "                            \"confidence\": confidence\n",
    "                        }\n",
    "                        rules.append(rule)\n",
    "    return rules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 13 association rules.\n",
      "{'825', '704'} -> {'39'} (conf=0.939, support=1035)\n",
      "{'39', '704'} -> {'825'} (conf=0.935, support=1035)\n",
      "{'825', '39'} -> {'704'} (conf=0.872, support=1035)\n",
      "{'704'} -> {'39'} (conf=0.617, support=1107)\n",
      "{'704'} -> {'825'} (conf=0.614, support=1102)\n",
      "{'227'} -> {'390'} (conf=0.577, support=1049)\n",
      "{'704'} -> {'825', '39'} (conf=0.577, support=1035)\n",
      "{'390'} -> {'227'} (conf=0.391, support=1049)\n",
      "{'390'} -> {'722'} (conf=0.388, support=1042)\n",
      "{'346'} -> {'217'} (conf=0.385, support=1336)\n",
      "{'825'} -> {'39'} (conf=0.385, support=1187)\n",
      "{'825'} -> {'704'} (conf=0.357, support=1102)\n",
      "{'825'} -> {'39', '704'} (conf=0.335, support=1035)\n"
     ]
    }
   ],
   "source": [
    "min_confidence = 0.3\n",
    "\n",
    "rules = generate_association_rules(frequent_itemsets, min_confidence)\n",
    "\n",
    "print(f\"Generated {len(rules)} association rules.\")\n",
    "\n",
    "rules_sorted = sorted(\n",
    "    rules,\n",
    "    key=lambda r: (r['confidence'], r['support']),\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "for rule in rules_sorted[:20]:\n",
    "    print(\n",
    "        f\"{set(rule['X'])} -> {set(rule['Y'])} \"\n",
    "        f\"(conf={rule['confidence']:.3f}, support={rule['support']})\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb18b26e",
   "metadata": {},
   "source": [
    "## ** Report \n",
    "\n",
    "## ** Instructions\n",
    "\n",
    "*put some instructions about the dataset like last time*\n",
    "\n",
    "## ** Introduction\n",
    "\n",
    "In this seminar, we implemented the A-Priori algorithm to identify frequent itemsets that occur in a set of transactions that have support over a given threshold. The support for a particular itemset is simply how many transactions in the dataset contain the itemset. Once those itemsets have been identified, we then generate assoication rules from those frequent itemsets to determine which rules have a high confidence. This process has applications in many domains, such as for vendors to determine whether certain purcahases may incline a customer to also include other items in thier purchase, or in pharmaceutical cases where side effects that come along with certain combinations of prescriptions can be identified.\n",
    "\n",
    "## ** Identifying Frequent Itemsets\n",
    "\n",
    "In order for an itemset to be frequent, it must appear iover a certain support threshold fraction of the transactions in the dataset. However, there are many itemsets to check out, especially once you get to itemsets with longer lengths. If you take a generous asumption of a transactions set that has 3000 unique items, that will have almost 500,000 unique pairs, and raise that to itemsets of length 3 and you get over 150 million unique triplets. Given the exponential size of possible itemsets, a simple method of naively checking how many transactions each itemset is a part of quickly becomes impossible.Due to this, we need a different method of identifying frequent itemsets that allows us to identify a much smaller set of \"likely\" itemsets that we can check through to identify frequent itemsets.\n",
    "\n",
    "This is where the A-Priori Algorithm comes into play. It is able to prune through and filter out many possible itemsets as not being frequent through the principle that a frequent itemset can only be that way if every subset is also a frequent itemset. For any itemset, adding another element to that itemset can only lower the ratio of transactions that it appears in, and so once an itemset becomes infrequent, any add-on to that base itemset will also remain infrequent. A-Priori utilizes this property, as when building the possible itemsets of length k that could be frequent, it only builds these candidate itemsets through combining the itemsets of length k-1 that were frequent, with frequent itemsets of length 1. This results in a far lower number of itemsets at each length that need to be checke. Instead of checking all possible pairs of, for instance, length 3 itemsets, it can build a smaller candidate pool by only checking the length 3 itemsets that result from a frequent length 2 itemset and a frequent singelton (length 1 itemset).\n",
    "\n",
    "We first generate the set L1, which is the set of all singleton itemsets that do meet the support threshold. From L1, we can then create the candidate list C2 for itemsets of length 2 from frequent itemsets in L1. Once C2 is created, the itemsets are then checked to see if they match the support requirement, and then the process repeats iteratively for C3, C4, and so on, continously building new candidate itemsets from the previous L(k-1). Doing this allows us to be able to identify frequent itemsets of larger degrees, whereas trying to check through all possible itemsets proves infeasible almost immediately. \n",
    "\n",
    "From testing on our dataset, at a supportability of .6 we notice it can take around 5-7 minutes to be able to identify all frequent itemsets, as despite pruning out infeasible candidate pairs that don't need to actively be considered throughout the process, there are still many candidate itemsets that can be created from our previous levels of frequent itemsets. From our dataset, the vast majority of the time is spent on creating itemsets of length 2, as with our support of .6, we have 375 length 1 itemsets identified, with comes out to around 70,000 different length 2 itemsets to check. After this however, our implementation is very quick at runtime to check candidate itemsets of higher lengths. This is due to the creation of k-size candidate sets being based off smaller length previous frequent itemsets, as this allows us to keep making our candidate sets smaller as our searched itemset size increases. As a result, this algorithm is applicable even on sets that can have larger size frequent itemsets, as each successive step has to go through less possible candidates.\n",
    "\n",
    "The support threshold also is a large factor in how long the a-priori algorithm takes, as setting a higher support threshold results in less starting length 1 itemsets that are identified as frequent, and so the algorithm will have less candidates at the C2 stage to construct. Increasing our support thereshold from 1000 to 2000 causes the initially identified frequent 1 pair itemsets to only be 155, and this decreases our C2 size drastically, and so as a result all frequent itemsets get identified in around 1 minute. Increasing supportability does impact the future association rule generation though, as less identified freuent itemsets allowes for less rules to be created and checked, when raising our supportbaility to 2000 for instance we now don't get any frequent itemsets of length > 1, and so as a result we cannot form any rules that meet our set supportability. As a result, the selected support threshold should depend on several factors: how large the dataset is and how feasible it is to run the algotihm based on the initial size of your C2 set, what level of supportability is considered significant in the context, and whether having rules with high confidence is paramount, rather than those rules having a high supportability being the important factor.\n",
    "\n",
    "## ** Generating Association Rules\n",
    "\n",
    "From frequent item sets, we can then generate association rules from them. The way that we generate these association rules is that for every itemset, we look at every proper subset P of that itemset (as a result itemset should be > 1 length), and we generate the rule that P -> {itemset}/P, and we check the confidence to see whether it meets our set confidence level. We guarantee that the association rules meet the support criteria, as we generate all rules from frequent itemsets, and so as a result for a generated X -> Y rule, X U Y is going to be some frequent itemset, which by definition meets our support requirement. \n",
    "\n",
    "From testing on our dataset, we notice that the confidence level has a very high impact on the identified association rules that meet the confidence criteria. When testing with a supportability of .60 for identifying frequent itemsets, we were able to get 9 itemsets of length 2 and 1 itemset of length 3, which gives us 24 total possible association rules. As a result, it does not take any considerable time to generate association rules given the low number of total possible rules, and this is as a result from being able to succesfully prune out infrequent itemsets previously, allowing us to build association rules that have a support above our accepted threshold. From testing with different confidence interval, the percentage of rules that met the confidence criteria varied heavily with the confidence threshold: \n",
    "\n",
    "| Confidence Threshold | # Of Generated Rules | Percentage of Rules Meeting Confidence |\n",
    "|----------------------|----------------------|----------------------------------------|\n",
    "|         0.9          |           2          |                 0.083                  |\n",
    "|         0.8          |           3          |                 0.125                  |\n",
    "|         0.6          |           5          |                 0.208                  |\n",
    "|         0.5          |           7          |                 0.291                  |\n",
    "|         0.3          |           13         |                 0.541                  |\n",
    "\n",
    "From this we can see that the percentage of rules that meet a criteria steadily increases as confidence decreases, which shows that the confidence of a rule is not weighed heavily towards being very high or very low, and is more evenly distributed from 0 to 1. Due to confidence not being skewed towards very high or very low values, slightly low confidence readings of .3-.6 may not be sufficient to be able to follow, as they are quite normal in our distribution of confidence levels and may not suggest any consistent pattern or trend. The selected confidence that is acceptable is dependent onthe environment as well, for something like retail where it may not be too costly to put certain items close together and more liberties can be taken to increase sales, lower confidence and less consistency in a pattern might be alright and enough to still group certain items together in a section of the store. However for other applications where the rules that are trusted to be consistent need to be very consistent, higher confidence values would need to be chosen to ensure loose rules are not followed as a pattern.\n",
    "\n",
    "## ** Conclusion\n",
    "\n",
    "This was an insighftul experiment into how frequent itemsets can be reasonably obtained from a dataset, and how those tiemsets can be used to generate association rules that can form observations about our data. A simple analysis into the amount of possible itemsets quickly shows that it is imperative to find algorithms to be able to progressively tune out more and more possibilties as being infrequent from the beginning, and implementing A-Priori showed us a method of doing that, and helped with understanding the underlying set logic behind how it is able to go about that. Generating association rules also helped us to visualize how real world observations can then be made after identifying frequent itemsets, and how those observations will inately having high support due to the nature of how those rules were formed from identified frequent itemsets.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-mining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
